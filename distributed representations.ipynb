{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_2 (1).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gWw2cPzcovZz"},"source":["# Assignment 2\n","\n","**Due to**: 9th November, 2020\n","\n","**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n","\n","**Summary**: Word embeddings, from sparse to dense representations"]},{"cell_type":"markdown","metadata":{"id":"1xgH0v9hdTKa"},"source":["# Intro\n","\n","In this assignment we will explore text encoding techniques, spanning from sparse representations, such as bag-of-words, to dense representations.\n","\n","In particular, we will see:\n","\n","*   Building a vocabulary\n","*   Building a word-word co-occurrence matrix\n","*   Defining a similarity metric: cosine similarity\n","*   Embedding visualization and analysis of their semantic properties\n","*   Better sparse representations via PPMI weighting\n","*   Loading pre-trained dense word embeddings (Word2Vec, GloVe)\n","*   Checking out-of-vocabulary (OOV) terms\n","*   Handling OOV terms\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uUQAeC3gqKhJ"},"source":["# Initial Setup\n","\n","First of all, we need to import some useful packages that we will use during this hands-on session."]},{"cell_type":"code","metadata":{"id":"7_zjgza4qZYE"},"source":["# system packages\n","import os\n","import shutil\n","import sys\n","\n","# data and numerical management packages\n","import pandas as pd\n","import numpy as np\n","\n","\n","# useful during debugging (progress bars)\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rI_8CixDqhcm"},"source":["# [Part I] Sparse embeddings\n","\n","As you know, working with text inherently requires a conversion step, formally known as embedding, that simply allows us to pass from string-like text to corresponding numerical representation. \n","\n","One of the most notable embedding method is the bag-of-words one (BoW). We simply count the occurrence of each word in the given corpus so as to build some useful data structures (matrices) that may give us some general idea of how the dataset is organized. For example, we can check where a particular word appears or, in a reversed perspective, identify the most common terms in each given document.\n","\n","This type of reasoning is directly related to how meaning is assigned to words. In particular, it is the environment itself, enclosing a word, that gives a specific meaning to it. Thus, we look for numerical encoding methods that reflect such point of view.\n","\n","Before diving into embedding analysis, we need to prepare a dataset and, most importantly, extract a vocabulary!"]},{"cell_type":"markdown","metadata":{"id":"3MmTNaGpv6L_"},"source":["## Prepare a dataset for experiments\n","\n","We will use the IMDB dataset of previous assignment. As you already know, it is a dataset of 50k sentences used for sentiment analysis. In particular, half of them (25k) is labelled as containing positive sentiment, whereas the remaining half are sentences of negative polarity.\n","\n","Contrarily to first assignment, we will ignore sentiment labels and we will focus only on learning a proper word embedding representation."]},{"cell_type":"markdown","metadata":{"id":"mWWAMPeTzfSh"},"source":["### Download and extraction\n","\n","We start by downloading the dataset and extract it to a folder."]},{"cell_type":"code","metadata":{"id":"6VphzaMCxZps"},"source":["from urllib import request\n","import tarfile\n","\n","# Config\n","print(\"Current work directory: {}\".format(os.getcwd()))\n","\n","dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n","\n","if not os.path.exists(dataset_folder):\n","    os.makedirs(dataset_folder)\n","\n","url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n","\n","dataset_path = os.path.join(dataset_folder, \"Movies.tar.gz\")\n","\n","print(dataset_path)\n","\n","def download_dataset(download_path, url):\n","    if not os.path.exists(download_path):\n","        print(\"Downloading dataset...\")\n","        request.urlretrieve(url, download_path)\n","        print(\"Download complete!\")\n","\n","def extract_dataset(download_path, extract_path):\n","    print(\"Extracting dataset... (it may take a while...)\")\n","    with tarfile.open(download_path) as loaded_tar:\n","        loaded_tar.extractall(extract_path)\n","    print(\"Extraction completed!\")\n","\n","# Download\n","download_dataset(dataset_path, url)\n","\n","# Extraction\n","extract_dataset(dataset_path, dataset_folder)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Naaatxml3idO"},"source":["Feel free to check the dataset folder content. Usually, the README file is a good starting point (if it exists and its well done, which is not so common!).\n","\n","Just like in the first assignment, we need a high level view of the dataset that is helpful to our needs. Thus, we will encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."]},{"cell_type":"code","metadata":{"id":"WvPbnTfOSvbT","executionInfo":{"status":"ok","timestamp":1604945784933,"user_tz":-60,"elapsed":39515,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"5be22430-c2b3-47b2-bbe4-a4bf5af851ba","colab":{"base_uri":"https://localhost:8080/"}},"source":["\n","# Config\n","dataset_name = \"aclImdb\"\n","debug = True\n","\n","def encode_dataset(dataset_name, debug=True):\n","    dataframe_rows = []\n","\n","    for split in tqdm(['train', 'test']):\n","        for sentiment in ['pos', 'neg']:\n","            folder = os.path.join(os.getcwd(), \"Datasets\", dataset_name, split, sentiment)\n","            for filename in os.listdir(folder):\n","                file_path = os.path.join(folder, filename)\n","                try:\n","                    if os.path.isfile(file_path):\n","                        # open the file\n","                        with open(file_path, mode='r', encoding='utf-8') as text_file:\n","                            # read it and extract informations\n","                            text = text_file.read()\n","                            score = filename.split(\"_\")[1].split(\".\")[0]\n","                            file_id = filename.split(\"_\")[0]\n","\n","                            num_sentiment = -1\n","\n","                            if sentiment == \"pos\" : num_sentiment = 1\n","                            elif sentiment == \"neg\" : num_sentiment = 0\n","\n","                            # create single dataframe row\n","                            dataframe_row = {\n","                                \"file_id\": file_id,\n","                                \"score\": int(score),\n","                                \"sentiment\": num_sentiment,\n","                                \"split\": split,\n","                                \"text\": text\n","                            }\n","\n","                            # print detailed info for the first file\n","                            if debug:\n","                                print(file_path)\n","                                print(filename)\n","                                print(file_id)\n","                                print(text)\n","                                print(score)\n","                                print(sentiment)\n","                                print(split)\n","                                print(dataframe_row)\n","                                debug = False\n","                            dataframe_rows.append(dataframe_row)\n","\n","                except Exception as e:\n","                    print('Failed to process %s. Reason: %s' % (file_path, e))\n","                    sys.exit(0)\n","\n","    folder = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", dataset_name)\n","    if not os.path.exists(folder):\n","        os.makedirs(folder)\n","\n","    # transform the list of rows in a proper dataframe\n","    df = pd.DataFrame(dataframe_rows)\n","    df = df[[\"file_id\",\n","                        \"score\",\n","                        \"sentiment\",\n","                        \"split\",\n","                        \"text\"]]\n","    dataframe_path = os.path.join(folder, dataset_name + \".pkl\")\n","    df.to_pickle(dataframe_path)\n","\n","    return df\n","\n","\n","# Encoding\n","print(\"Encoding dataset...\")\n","df = encode_dataset(dataset_name, debug)\n","print(\"Encoding completed!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Encoding dataset...\n","/content/Datasets/aclImdb/train/pos/2112_9.txt\n","2112_9.txt\n","2112\n","In Budapest, Margaret Sullavan (as Klara Novak) gets a job as clerk in a gift shop; there, she bickers with co-worker James Stewart (as Alfred Kralik). The two don't get along on the job because each has fallen in love with a unseen pen pal. Watching Ernst Lubitsch direct these stars through the inevitable is predictably satisfying. <br /><br />Even better is a sub-plot involving shop owner Frank Morgan (as Hugo Matuschek), who suspects his wife is having an affair. Hiring a private detective, Mr. Morgan confirms his wife of 22 years is having sex with one of his younger employees. Morgan, painfully realizing, \"She just didn't want to grow old with me,\" and the supporting characters are what keeps this film from getting old.<br /><br />********* The Shop Around the Corner (1/12/40) Ernst Lubitsch ~ James Stewart, Margaret Sullavan, Frank Morgan, Joseph Schildkraut\n","9\n","pos\n","train\n","{'file_id': '2112', 'score': 9, 'sentiment': 1, 'split': 'train', 'text': 'In Budapest, Margaret Sullavan (as Klara Novak) gets a job as clerk in a gift shop; there, she bickers with co-worker James Stewart (as Alfred Kralik). The two don\\'t get along on the job because each has fallen in love with a unseen pen pal. Watching Ernst Lubitsch direct these stars through the inevitable is predictably satisfying. <br /><br />Even better is a sub-plot involving shop owner Frank Morgan (as Hugo Matuschek), who suspects his wife is having an affair. Hiring a private detective, Mr. Morgan confirms his wife of 22 years is having sex with one of his younger employees. Morgan, painfully realizing, \"She just didn\\'t want to grow old with me,\" and the supporting characters are what keeps this film from getting old.<br /><br />********* The Shop Around the Corner (1/12/40) Ernst Lubitsch ~ James Stewart, Margaret Sullavan, Frank Morgan, Joseph Schildkraut'}\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Encoding completed!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eHCroMaDdQdB"},"source":["### Loading and Visualization\n","\n","The next step is to load the dataset and inspect some of its elements in order to have an idea of the general content. We will use **pandas** library for dataset loading as follows."]},{"cell_type":"code","metadata":{"id":"9itQUTUA3e_C","executionInfo":{"status":"ok","timestamp":1604945784935,"user_tz":-60,"elapsed":39507,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"fe5856ed-8034-4374-a1d9-6ae912895173","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Inspection\n","\n","print(\"Dataset size: {}\".format(df.shape)) # (50000, 5)\n","print(\"Dataset columns: {}\".format(df.columns.values)) # ['file_id', 'score', 'sentiment', 'split', 'text]\n","\n","print(\"Classes distribution:\\n{}\".format(df.sentiment.value_counts())) # [0: 25000, 1: 25000]\n","\n","print(\"Some examples: {}\".format(df.iloc[:5]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset size: (50000, 5)\n","Dataset columns: ['file_id' 'score' 'sentiment' 'split' 'text']\n","Classes distribution:\n","1    25000\n","0    25000\n","Name: sentiment, dtype: int64\n","Some examples:   file_id  score  ...  split                                               text\n","0    2112      9  ...  train  In Budapest, Margaret Sullavan (as Klara Novak...\n","1    4090      8  ...  train  This movie is simply incredible! I had expecte...\n","2    7877      7  ...  train  The Write Word<br /><br />What you see is what...\n","3    2630      8  ...  train  \"The Lion King\" is without a doubt my favorite...\n","4    3641      9  ...  train  This movie has several things going for it. It...\n","\n","[5 rows x 5 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QuuACGZs5fLS"},"source":["Feel free to inspect the dataset as you wish in the following code space!"]},{"cell_type":"code","metadata":{"id":"svU6BZUf5jCH"},"source":["\"\"\" YOUR CODE HERE \"\"\"\n","df.groupby([\"sentiment\",\"score\"]).count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzdI49GOv_p1"},"source":["count_score = df.groupby(\"score\").count()\n","count_score.loc[:,\"text\"].plot.bar()\n","count_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTYo05YVeu7e"},"source":["### [Optional] A quick simplification\n","\n","Since the dataset is quite large, the embedding related methods, such as co-occurrence matrix construction, may take a while or may require ad hoc solutions. For instance, if we consider the whole dataset (50k sentences) the vocabulary should be around 160k terms and we don't have sufficient memory to load a (160k, 160k) co-occurrence matrix.\n","\n","For the purpose of this assignment, we can rely on a small slice of the dataset.\n","In this way, we can get results in small amount of time. Nonetheless, feel free\n","to work with the whole dataset! Suggestions on how to handle this scenario are\n","given below when required.\n","\n","Select the amount of dataset samples you want to keep and re-define the dataset as follows."]},{"cell_type":"code","metadata":{"id":"AyghmotufcFe","executionInfo":{"status":"ok","timestamp":1604945784939,"user_tz":-60,"elapsed":39471,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"13259207-6561-401a-d8d4-9a05446b9ef6","colab":{"base_uri":"https://localhost:8080/"}},"source":["samples_amount = 1000\n","\n","# This type of slicing is not mandatory,\n","# but it is sufficient to our purposes\n","np.random.seed(42)\n","random_indexes = np.random.choice(np.arange(df.shape[0]),\n","                                  size=samples_amount,\n","                                  replace=False)\n","\n","df = df.iloc[random_indexes]\n","\n","print('New dataset size: ', df.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["New dataset size:  (1000, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ohww_Fda5wR-"},"source":["## Building the Vocabulary\n","\n","At this point we can build the word vocabulary of our dataset. This information is the first step of any word embedding method: we need to know the set of atomic entities that build up our corpus.\n","\n","**Definition**: a vocabulary is a collection of words occurring in a given dataset. More precisely, each word is recognized and assigned an index.\n","\n","**Example**: Suppose you have the given toy corpus $D$: { \"the cat is on the table\" }\n","\n","As you notice, the dataset is comprised of only one sentence: \"the cat is on the table\". The corresponding vocabulary (a possible one) will be:\n","\n","V = {0: 'the', 1: 'cat', 2: 'is', 3: 'on', 4: 'table'}\n","\n","In this case, indexing follows word order, but it is not mandatory!\n","\n","**Important**: The most important thing to remember is that the vocabulary should always be the same one. Thus, make sure that the vocabulary creation routine always returns the same result!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VYOu4FV067O_"},"source":["### Some Cleaning\n","\n","Before vocabulary creation, we have to do a little bit of text pre-processing so as to avoid spurious data.\n","\n","Pre-processing is always an important step in any machine learning based task, since data quality is one of the crucial factors that lead to better performance. Models, even state-of-the-art ones, hardly achieve satisfying results if the dataset is very noisy.\n","\n","**Types of pre-processing**: there are a lot of pre-processing steps that we can consider, either general or quite task- specific. Here we will rely on very standard and simple methods.\n","\n","*    **Text to lower**: casing usually doesn't affect our task, but in some scenarios, such as part-of-speech tagging, might even be crucial.\n","\n","*    **Replace special characters**: special characters are usually employed as variants of a single character like the spacing symbol ' '. \n","In other cases (dates, etc..) special characters might have a specific meaning and should not be replaced.\n","\n","*    **Text stripping**: it is important to filter out extra spaces to avoid unwanted distinctions between identical words, such as 'apple' and ' apple '.\n","\n","There are a lot of pre-processing techniques, such as number replacing, lemmatization, stemming, spell correction, acronyms merge and so on. If you are interested you can check [here](https://medium.com/swlh/text-normalization-7ecc8e084e31) and [here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) some good blogs about the topic.\n","\n","**NOTE**: If you feel like there should be some additional pre-processing, feel free to modify this section as you please! Please, remember to provide additional comments to motivate your changes."]},{"cell_type":"code","metadata":{"id":"2TLTu0-2JQwi"},"source":["import re\n","from functools import reduce\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# Config\n","\n","REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","try:\n","    STOPWORDS = set(stopwords.words('english'))\n","except LookupError:\n","    nltk.download('stopwords')\n","    STOPWORDS = set(stopwords.words('english'))\n","\n","def lower(text):\n","    \"\"\"\n","    Transforms given text to lower case.\n","    Example:\n","    Input: 'I really like New York city'\n","    Output: 'i really like new your city'\n","    \"\"\"\n","\n","    return text.lower().replace('br', ' ')\n","\n","def replace_special_characters(text):\n","    \"\"\"\n","    Replaces special characters, such as paranthesis,\n","    with spacing character\n","    \"\"\"\n","\n","    return REPLACE_BY_SPACE_RE.sub(' ', text)\n","\n","def filter_out_uncommon_symbols(text):\n","    \"\"\"\n","    Removes any special character that is not in the\n","    good symbols list (check regular expression)\n","    \"\"\"\n","\n","    return GOOD_SYMBOLS_RE.sub('', text)\n","\n","def remove_stopwords(text):\n","    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n","\n","\n","def strip_text(text):\n","    \"\"\"\n","    Removes any left or right spacing (including carriage return) from text.\n","    Example:\n","    Input: '  This assignment is cool\\n'\n","    Output: 'This assignment is cool'\n","    \"\"\"\n","\n","    return text.strip()\n","\n","PREPROCESSING_PIPELINE = [\n","                          lower,\n","                          replace_special_characters,\n","                          filter_out_uncommon_symbols,\n","                          remove_stopwords,\n","                          strip_text\n","                          ]\n","\n","# Anchor method\n","\n","def text_prepare(text, filter_methods=None):\n","    \"\"\"\n","    Applies a list of pre-processing functions in sequence (reduce).\n","    Note that the order is important here!\n","    \"\"\"\n","\n","    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n","\n","    return reduce(lambda txt, f: f(txt), filter_methods, text)\n","\n","# Pre-processing\n","\n","print('Pre-processing text...')\n","\n","print()\n","print('[Debug] Before:\\n{}'.format(df.text[:3]))\n","print()\n","\n","# Replace each sentence with its pre-processed version\n","df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n","\n","print('[Debug] After:\\n{}'.format(df.text[:3]))\n","print()\n","\n","print(\"Pre-processing completed!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YphBz_iaMRm0"},"source":["### **Vocabulary Creation**\n","\n","We are now ready to create the vocabulary! This task is up to you! Complete the below function and remember to follow mentioned requirements.\n","\n","FYI, since the text has been pre-processed, space splitting should work correctly. \n","\n","Bare in mind that some packages offers tools for automatic vocabulary creation, such as Keras (check [keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)). \n","\n","**NOTE**: It is not mandatory to use the keras Tokenizer, we mention it so that you know there exist tools specific for this step.\n","\n","**NOTE**: In this case, the vocabulary will start from index equal to 1, since the 0 slot is reserved to padding token. In order to pass vocabulary evaluation, you have to re-scale the vocabulary such that the first index is 0.\n","\n","**NOTE**: If you are using the keras Tokenizer, remember to use its method <code> texts_to_sequences </code> for splitting, otherwise you might find terms that are not in the vocabulary! Please, check also its constructor argument <code> filters </code> since it defines a pre-processing regexp.\n","\n","This easy task is just to let you know that is important to check the built vocabulary just to make sure everything is ok."]},{"cell_type":"code","metadata":{"id":"hRs7m-SxMq0n"},"source":["from collections import OrderedDict\n","\n","# Function definition\n","def build_vocabulary(df):\n","    \"\"\"\n","    Given a dataset, builds the corresponding word vocabulary.\n","\n","    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n","    :return:\n","      - word vocabulary: vocabulary index to word\n","      - inverse word vocabulary: word to vocabulary index\n","      - word listing: set of unique terms that build up the vocabulary\n","    \"\"\"\n","    ### YOUR CODE HERE ###\n","    import itertools\n","    text =  df.loc[:, 'text'] # get the text to be processed\n","    bagOfWords = [txt.split(' ') for txt in text]  # get bag of words\n","    bagOfWords = list(itertools.chain(*bagOfWords))  # flatten the bagofwords\n","    word_to_idx = OrderedDict()\n","    for w in bagOfWords:\n","      if not w in word_to_idx.keys():\n","        word_to_idx[w] = 1\n","      else: word_to_idx[w] +=1\n","    unique_w = word_to_idx.keys()\n","    \n","    idx_to_word = OrderedDict(zip([i for i,_ in enumerate(unique_w)],unique_w))\n","    word_to_idx = OrderedDict(zip(unique_w, [i for i,_ in enumerate(unique_w)]))\n","    return idx_to_word, word_to_idx, list(unique_w)\n"," \n","\n","\n","# Testing\n","idx_to_word, word_to_idx, word_listing = build_vocabulary(df)\n","\n","print('[Debug] Index -> Word vocabulary size: {}'.format(len(idx_to_word)))\n","print('[Debug] Word -> Index vocabulary size: {}'.format(len(word_to_idx)))\n","\n","print('[Debug] Some words: {}'.format([(idx_to_word[idx], idx) for idx in np.arange(10) + 1]))\n","\n","# Evaluation\n","\n","def evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df, check_default_size=False):\n","\n","    # Check size\n","    print(\"[Vocabulary Evaluation] Size checking...\")\n","\n","    assert len(idx_to_word) == len(word_to_idx)\n","    assert len(idx_to_word) == len(word_listing)\n","\n","    # Check content\n","    print(\"[Vocabulary Evaluation] Content checking...\")\n","\n","    for i in tqdm(range(0, len(idx_to_word))):\n","        assert idx_to_word[i] in word_to_idx\n","        assert word_to_idx[idx_to_word[i]] == i\n","\n","    # Check consistency\n","    print(\"[Vocabulary Evaluation] Consistency checking...\")\n","\n","    _, _, first_word_listing = build_vocabulary(df)\n","    _, _, second_word_listing = build_vocabulary(df)\n","    assert first_word_listing == second_word_listing\n","\n","    # Check toy example\n","    print(\"[Vocabulary Evaluation] Toy example checking...\")\n","    toy_df = pd.DataFrame.from_dict({\n","        'text': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n","    })\n","    _, _, toy_word_listing = build_vocabulary(toy_df)\n","    toy_valid_vocabulary = set(' '.join(toy_df.text.values).split())\n","    assert set(toy_word_listing) == toy_valid_vocabulary\n","\n","\n","print(\"Vocabulary evaluation...\")\n","evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df)\n","print(\"Evaluation completed!\") "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSO2NA5ZYnz_"},"source":["Feel free to inspect the vocabulary! To this purpose, you can use the following code space."]},{"cell_type":"code","metadata":{"id":"wT4oB5PcYyd1","executionInfo":{"status":"ok","timestamp":1604945786989,"user_tz":-60,"elapsed":41488,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"cd164464-03df-4df1-a2a8-df58581d0b5d","colab":{"base_uri":"https://localhost:8080/"}},"source":["\"\"\" YOUR CODE HERE \"\"\"\n","_,_,vocab = build_vocabulary(df)\n","vocab[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['sicily',\n"," 'territory',\n"," 'baroque',\n"," 'doubling',\n"," 'perspective',\n"," 'thats',\n"," 'part',\n"," 'movies',\n"," 'challenge',\n"," 'realism']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"3pUTYnmcFQct"},"source":["### **Save the vocabulary**\n","\n","Generally speaking, it is a good idea to save the dictionary in clear format. In this way you can quickly check for errors or useful words.\n","\n","In this case, we will save the vocabulary dictionary in JSON format."]},{"cell_type":"code","metadata":{"id":"mg41avgpFlFJ","executionInfo":{"status":"ok","timestamp":1604945790688,"user_tz":-60,"elapsed":45176,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"97a010d5-51c3-4325-ac41-b80d1f0919c1","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install simplejson\n","import simplejson as sj\n","\n","vocab_path = os.path.join(os.getcwd(), 'Datasets', dataset_name, 'vocab.json')\n","\n","print(\"Saving vocabulary to {}\".format(vocab_path))\n","with open(vocab_path, mode='w') as f:\n","    sj.dump(word_to_idx, f, indent=4)\n","print(\"Saving completed!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting simplejson\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n","\r\u001b[K     |██▋                             | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.3MB/s \n","\u001b[?25hInstalling collected packages: simplejson\n","Successfully installed simplejson-3.17.2\n","Saving vocabulary to /content/Datasets/aclImdb/vocab.json\n","Saving completed!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WrWb80lyY629"},"source":["## Building the Co-occurence Matrix\n","\n","As we said at the beginning, embedding methods are based on the principle that similar words will be used in similar contexts. Thus, context information is crucial to determine the meaning of a word.\n","\n","One basic approach, which falls under the category of sparse representations, is the **co-occurrence matrix**: for each word in the vocabulary we count the number of times each other word appears within the same context window. A simple example is given by image below.\n","\n","![](https://drive.google.com/uc?export=view&id=1UknGoYvIBBA7ytkSlqm1NhF_lHt0iOwT)\n","\n","In particular, the context window defines our notion of word context. Consider the following example:\n","\n","<h3><center> The cat is on the table </center></h3>\n","\n","We have to consider each word in the sentence and for each we have count words within the context window. Suppose a window of size 2, then we have for the word 'cat':\n","\n","Current word: cat\n","\n","Context words: [the, is, on]\n","\n","Notice how we consider $W$ words back and ahead of current word, where $W$ is the window size.\n","\n","\n","\n","\n","Let's define the simplest version of a **co-occurrence matrix** based on word counting.\n","\n","---\n","\n","**Small dataset case**: If you selected a small slice of the dataset, you should have a vocabulary size that we can afford in terms of memory demand. Thus, you can easily instantiate the co-occurrence matrix and populate it iteratively.\n","\n","---\n","\n","**Large dataset case**: We have to work with sparse matrices due to the high vocabulary size and to the low amount of non-zero word counts. To this end, the [Scipy package](https://docs.scipy.org/doc/scipy/reference/sparse.html) allows us to easily define sparse matrices that can be converted ot numpy arrays (if we can).\n","\n","**Suggestion**: The simplest way to build the co-occurrence matrix is via an incremental approach: we loop through dataset sentences, split into words and then count co-occurrences within the given window frame. Generally, combining this approach with sparse matrices is not so efficient (yet possible). However, Scipy offers [$\\texttt{lil_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix) sparse format that is suitable to this case. Anyway, check out other sparse formats, such as [$\\texttt{csr_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), and the corresponding building methods.\n","\n","Working with $\\texttt{lil_matrix}$ might take $\\sim 1h$ of time to build the whole dataset co-occurrence matrix. It is also possibile to work with $\\texttt{csr_matrix}$ but the approach is more complex (check the last example of the corresponding documentation page).\n"]},{"cell_type":"code","metadata":{"id":"l75bed97MpBa"},"source":["def window0(L,w=2, sort=False):\n","  res = []\n","  for i in range(len(L)):\n","    current_el = L[i]\n","    next_element = L[max(0,i-w):i] + L[i+1:i+w+1]  \n","    for j in next_element:\n","      if sort:\n","        res.append(tuple(sorted([current_el,j])))\n","      else: res.append(tuple([current_el,j])) \n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OS5H65sh52b9","executionInfo":{"status":"ok","timestamp":1604945824486,"user_tz":-60,"elapsed":78961,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"ac134d34-b807-45ec-91ff-f530c79b747e","colab":{"base_uri":"https://localhost:8080/"}},"source":["import scipy.sparse    # defines several types of efficient sparse matrices\n","import zipfile\n","import gc\n","import requests\n","import time\n","import itertools\n","from collections import Counter, OrderedDict\n","# Function definition\n","\n","def co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4):\n","    \"\"\"\n","    Builds word-word co-occurrence matrix based on word counts.\n","\n","    :param df: pre-processed dataset (pandas.DataFrame)\n","    :param idx_to_word: vocabulary map (index -> word) (dict)\n","    :param word_to_idx: vocabulary map (word -> index) (dict)\n","\n","    :return\n","      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n","    \"\"\"\n","\n","    ### YOUR CODE HERE ###\n","    data = df\n","    _,_,unique_words = build_vocabulary(data)\n","    text = data.loc[:, 'text'] # get the text to be processed\n","    bagOfWords0 = [txt.split() for txt in text]  # get bag of words\n","    bagOfWords0 = [window0(d,window_size) for d in bagOfWords0]\n","    bagOfWords = list(itertools.chain(*bagOfWords0))  # flatten the bagofwords\n","    d = Counter()\n","    for key in bagOfWords:\n","      d[key] += 1\n","    \n","    d = dict(d)\n","    \n","    v = len(unique_words)\n","    #dff = np.zeros((v,v), dtype=np.int64)\n","    \n","    dff = scipy.sparse.lil_matrix((v,v), dtype=np.int16)\n","    for k,v in d.items():\n","      dff[word_to_idx[k[0]],word_to_idx[k[1]]] = v\n","      dff[word_to_idx[k[1]],word_to_idx[k[0]]] = v\n","\n","    return dff.toarray()\n","\n","\n","# Testing\n","window_size = 4\n","\n","# Clean RAM before re-running this code snippet to avoid session crash\n","if 'co_occurrence_matrix' in globals():\n","    del co_occurrence_matrix\n","    gc.collect()\n","    time.sleep(10.)\n","idx_to_word,word_to_idx,_=build_vocabulary(df)\n","print(\"Building co-occurrence count matrix... (it may take a while...)\")\n","co_occurrence_matrix = co_occurrence_count(df, idx_to_word, word_to_idx, window_size)\n","print(\"Building completed!\")\n","\n","\n","# Evaluation\n","\n","def save_response_content(response, destination):\n","    CHUNK_SIZE = 32768\n","\n","    with open(destination, \"wb\") as f:\n","        for chunk in response.iter_content(CHUNK_SIZE):\n","            if chunk: # filter out keep-alive new chunks\n","                f.write(chunk)\n","\n","def download_toy_data(benchmark_path):\n","    toy_data_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark.zip')\n","    toy_data_url_id = \"1z8qp034utvW7kv-9Q_TACJv3_sdCzkZg\"\n","    toy_url = \"https://docs.google.com/uc?export=download\"\n","\n","    if not os.path.exists(benchmark_path):\n","        os.makedirs(benchmark_path)\n","\n","    if not os.path.exists(toy_data_path):\n","        print(\"Downloading co-occurrence count matrix benchmark data...\")\n","        with requests.Session() as current_session:\n","            response = current_session.get(toy_url,\n","                                   params={'id': toy_data_url_id},\n","                                   stream=True)\n","        save_response_content(response, toy_data_path)\n","        print(\"Download complete!\")\n","\n","        print(\"Extracting dataset...\")\n","        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n","            loaded_zip.extractall(benchmark_path)\n","        print(\"Extraction complete!\")\n","\n","def evaluate_co_occurrence_matrix(matrix):\n","    is_sparse = False\n","\n","    if hasattr(scipy.sparse, type(matrix).__name__):\n","        print(\"Detected sparse co-occurrence matrix!\")\n","        is_sparse = True\n","\n","    # Check symmetry\n","    print(\"[Co-occurrence count matrix Evaluation] Symmetry checking...\")\n","    if is_sparse:\n","        assert (matrix != matrix.transpose()).nnz == 0\n","    else:\n","        assert np.equal(matrix, matrix.transpose()).all()\n","\n","    # Check toy example\n","    print(\"[Co-occurrence count matrix Evaluation] Toy example checking...\")\n","    toy_df = pd.DataFrame.from_dict({\n","        'text': [\"all that glitters is not gold\",\n","                 \"all in all i like this assignment\"],\n","    })\n","    benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n","    toy_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark')\n","    download_toy_data(benchmark_path)\n","\n","    toy_idx_to_word = np.load(os.path.join(toy_path, 'toy_idx_to_word.npy'), allow_pickle=True).item()\n","    toy_word_to_idx = np.load(os.path.join(toy_path, 'toy_word_to_idx.npy'), allow_pickle=True).item()\n","\n","    toy_matrix = co_occurrence_count(toy_df, toy_idx_to_word, toy_word_to_idx, window_size=1)\n","    toy_valid_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_count.npy'))\n","\n","    if is_sparse:\n","        assert np.equal(toy_matrix.todense(), toy_valid_matrix).all()\n","    else:\n","        assert np.equal(toy_matrix, toy_valid_matrix).all()\n","\n","\n","print(\"Evaluating co-occurrence matrix\")\n","evaluate_co_occurrence_matrix(co_occurrence_matrix)\n","print(\"Evaluation completed!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Building co-occurrence count matrix... (it may take a while...)\n","Building completed!\n","Evaluating co-occurrence matrix\n","[Co-occurrence count matrix Evaluation] Symmetry checking...\n","[Co-occurrence count matrix Evaluation] Toy example checking...\n","Downloading co-occurrence count matrix benchmark data...\n","Download complete!\n","Extracting dataset...\n","Extraction complete!\n","Evaluation completed!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"So0i7CWy-fAW"},"source":["### **Got Stuck?**\n","\n","If you are stuck, but still want to try out following sections, you can experiment with a valid co-occurrence matrix provided by us as follows"]},{"cell_type":"code","metadata":{"id":"pJWDWacd-vrG","executionInfo":{"status":"ok","timestamp":1604945824857,"user_tz":-60,"elapsed":79321,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"e3149c8b-4bf7-45a6-91b6-352819f16c1c","colab":{"base_uri":"https://localhost:8080/"}},"source":["benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n","valid_data_benchmark_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark', \"{}.npy\")\n","\n","download_toy_data(benchmark_path)\n","\n","co_occurrence_matrix = np.load(valid_data_benchmark_path.format('valid_co-occurrence_matrix_count'))\n","idx_to_word = np.load(valid_data_benchmark_path.format('valid_idx_to_word'), allow_pickle=True).item()\n","word_to_idx = np.load(valid_data_benchmark_path.format('valid_word_to_idx'), allow_pickle=True).item()\n","word_listing = np.load(valid_data_benchmark_path.format('valid_word_listing'))\n","\n","print('Co-occurrence matrix shape: ', co_occurrence_matrix.shape)\n","print('Index -> word vocabulary size: ', len(idx_to_word))\n","print('Word -> index vocabulary size: ', len(word_to_idx))\n","print('Word listing size: ', len(word_listing))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Co-occurrence matrix shape:  (14244, 14244)\n","Index -> word vocabulary size:  14244\n","Word -> index vocabulary size:  14244\n","Word listing size:  14244\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H10Eqj5-CyZv"},"source":["## Embedding Visualization\n","\n","The next step is to visualize our sparse word embeddings in a lower dimensional space (2D) in order to have an idea of the meaning attributed to each word.\n","\n","**How?** Well, there are some dimensionality reduction techniques that we might employ. We will explore SVD and t-SNE methods, without delving into technical details since they are not arguments of this NLP course.\n","\n","**SVD Memo**: SVD stands for **Singular Value Decomposition** and is a kind of generalized **Principal Components Analysis** (PCA) and focuses on selecting the top **k** principal components. For more info, [here](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf) you can find a brief tutorial.\n","\n","**t-SNE Memo**: t-SNE stands for **t-Distributed Stochastic Neighbour Embedding** and is an unsupervised non-linear technique. The non-linearity is one major point of difference with PCA. Additionally, it preserves small pairwise distance (or local similarities), whereas PCA aims to preserve large pairwise distances in order to maximize variance. The basic idea of t-SNE is to compute a similarity measure between a pair of instances both at high and low dimensional space and optimize these two similarities via a cost function. Properly using t-SNE is a bit tricky, a well recommended reading is one of the [author's blog](https://lvdmaaten.github.io/tsne/).\n","\n","**Note**: We strongly suggest you to play with the window size and check if there are some notable differences. Generally, a small window size reflects syntactic properties, while a large window size captures semantic ones."]},{"cell_type":"code","metadata":{"id":"qNzFd1-4JBvS"},"source":["from sklearn.manifold import TSNE\n","from sklearn.decomposition import TruncatedSVD\n","import matplotlib.pyplot as plt\n","\n","# Function definition\n","\n","def visualize_embeddings(embeddings, word_annotations=None, word_to_idx=None):\n","    \"\"\"\n","    Plots given reduce word embeddings (2D).\n","    Users can highlight specific words (word_annotations list) in order to better\n","    analyse the effectiveness of the embedding method.\n","\n","    :param embeddings: word embedding matrix of shape (words, 2) retrieved via a\n","                       dimensionality reduction technique.\n","    :param word_annotations: list of words to be annotated.\n","    :param word_to_idx: vocabulary map (word -> index) (dict)\n","    \"\"\"\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n","\n","    if word_annotations:\n","        print(\"Annotating words: {}\".format(word_annotations))\n","\n","        word_indexes = []\n","        for word in word_annotations:\n","            word_index = word_to_idx[word]\n","            word_indexes.append(word_index)\n","\n","        word_indexes = np.array(word_indexes)\n","\n","        other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n","        target_embeddings = embeddings[word_indexes]\n","\n","        ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n","        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n","        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n","\n","        for word, word_index in zip(word_annotations, word_indexes):\n","            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n","            ax.annotate(word, xy=(word_x, word_y))\n","\n","    else:\n","        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n","\n","    # Set proper axis limit range\n","    # We avoid outliers ruining the visualization if they are quite far away\n","    xmin_quantile = np.quantile(embeddings[:, 0], q=0.01)\n","    xmax_quantile = np.quantile(embeddings[:, 0], q=0.99)\n","\n","    ymin_quantile = np.quantile(embeddings[:, 1], q=0.01)\n","    ymax_quantile = np.quantile(embeddings[:, 1], q=0.99)\n","\n","    ax.set_xlim(xmin_quantile, xmax_quantile)\n","    ax.set_ylim(ymin_quantile, ymax_quantile)\n","\n","\n","def reduce_SVD(embeddings):\n","    \"\"\"\n","    Applies SVD dimensionality reduction.\n","\n","    :param embeddings: word embedding matrix of shape (words, dim). In the case\n","                       of a word-word co-occurrence matrix the matrix shape would\n","                       be (words, words).\n","\n","    :return\n","        - 2-dimensional word embedding matrix of shape (words, 2)\n","    \"\"\"\n","  \n","    print(\"Running SVD reduction method...\")\n","    svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n","    reduced = svd.fit_transform(embeddings)\n","    print(\"SVD reduction completed!\")\n","\n","    return reduced\n","\n","# Note: this method may take a while\n","def reduce_tSNE(embeddings):\n","    \"\"\"\n","    Applies t-SNE dimensionality reduction.\n","\n","    :param embeddings: word embedding matrix of shape (words, dim). In the case\n","                       of a word-word co-occurrence matrix the matrix shape would\n","                       be (words, words).\n","\n","    :return\n","        - 2-dimensional word embedding matrix of shape (words, 2)\n","    \"\"\"\n","\n","    print(\"Running t-SNE reduction method... (it may take a while...)\")\n","    tsne = TSNE(n_components=2, random_state=42, n_iter=1000, metric='cosine', n_jobs=2)\n","    reduced = tsne.fit_transform(embeddings)\n","    print(\"t-SNE reduction completed!\")\n","    print(reduced.shape)\n","\n","    return reduced\n","\n","# Testing\n","\n","# Feel free to play with word_annotations argument!\n","# Some suggestions: stopwords (if not removed), nouns, adjectives\n","# Check the saved dictionary!\n","\n","# SVD\n","reduced_SVD = reduce_SVD(co_occurrence_matrix)\n","visualize_embeddings(reduced_SVD, ['good', 'love', 'beautiful'], word_to_idx)\n","\n","# t-SNE\n","# Note: this method may take a while (just relax :-))\n","reduced_tSNE = reduce_tSNE(co_occurrence_matrix)\n","visualize_embeddings(reduced_tSNE, ['good', 'love', 'beautiful'], word_to_idx)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OjI9eByS8AKa"},"source":["Feel free to play with visualization!"]},{"cell_type":"code","metadata":{"id":"NfmB19Bu8C_H"},"source":["### YOUR CODE HERE ###\n","# SVD\n","reduced_SVD = reduce_SVD(co_occurrence_matrix)\n","visualize_embeddings(reduced_SVD, ['bad', 'first', 'king', 'prince','good'], word_to_idx)\n","\n","# t-SNE\n","# Note: this method may take a while (just relax :-))\n","reduced_tSNE = reduce_tSNE(co_occurrence_matrix)\n","visualize_embeddings(reduced_tSNE,['bad', 'first', 'king', 'prince','good'], word_to_idx)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxPTZE_U4qUk"},"source":["### Embedding properties\n","\n","Visualization can give us a rough idea of how word embeddings are organized and if some semantic properties are reflected in the numerical dimensional space. For example, are synonyms close together? Ideally, if the dataset is big enough, we should see similar vector embeddings since synonyms usually have similar contexts.\n","\n","**How to do that?** We could highlight target words in the visualization step and check if our expectations are met. For instance, synonyms should be close together. However, this method is rather inaccurate and time-consuming (dimensionality reduction is not a perfect mapping). Thus, we need some sort of similarity metric that is independent of the vector dimensionality."]},{"cell_type":"markdown","metadata":{"id":"ZrhbF-lEESyW"},"source":["#### **Cosine Similarity**\n","\n","Let us now consider again the matrix obtained using <code>co_occurrence_count</code>. \n","Since we want to meaure how two word vectors are far apart, a naive solution would involve computing the dot product of the two vectors. However, this metric will give higher similarity either to longer vectors or to vectors that have higher counts.\n","\n","A better metric is **cosine similarity** which is just a normalized dot product.\n","\n","$s(p, q) = \\frac{p \\, \\cdot \\, q}{||p|| \\, \\cdot \\, ||q||}$\n","\n","where $s(p, q) \\in [-1, 1] $, since it computes the cosine of the angle between the two vectors. Intuitively, we are bringing vectors down to the d-dimensional unit sphere (d is the vocab size) and then computing their distance (in 2D space we will have a circle).\n","\n","Now, write down the cosine similarity formula so that we can proceed testing word embedding properties!\n","\n","**NOTE**: Since we are working with matrices, we will ask you to define a cosine similarity function that works with matrices. Extending to matrices is quite easy if you think them as lists of vectors.\n","\n","**NOTE**: It is permitted to use functions of existing packages (e.g. sci-kit learn). This is mainly for efficiency motivation. We are not going to discriminate between such solutions and the ones that manually implement the cosine similarity metric (since it is not the main objective of the assignment).\n","\n","**WHAT YOU HAVE TO DO**: First of all, try to manually define the cosine similarity operation. If your implementation is correct but not so efficient, you can define a separate cosine similarity function that leverages implementations of existing packages like sci-kit learn. Here we want to verify if you understand the metric, but at the same time we don't want to penalize you if you are not as efficient as possible (although it is one factor that it is important to not forget when coding in general)."]},{"cell_type":"code","metadata":{"id":"VKco6M2Hf0ns"},"source":["from scipy.sparse import lil_matrix\n","\n","def cosine(p0,q0) -> np.float32:\n","    d = np.dot(p0,q0)\n","    return d/(norm0(p0)*norm0(q0))\n","\n","def norm0(p0) -> np.float32:\n","  p0 = p0**2\n","  return np.sqrt(np.sum(p0))\n","\n","\n","\n","def manually_computed_consine_similarity(p,q) -> np.ndarray:\n","  if len(p.shape)==1:\n","    p = p.reshape(1,-1)\n","  if len(q.shape)==1:\n","    q = q.reshape(1,-1)\n","  \n","  res = lil_matrix((p.shape[0],q.shape[0]))\n","  #res = np.zeros((p.shape[0],q.shape[0]))\n","  for i in range(p.shape[0]):\n","    for j in range(q.shape[0]):\n","      res[i,j] = cosine(p[i],q[j])\n","  return res.toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUSa7FtAOiMQ","executionInfo":{"status":"ok","timestamp":1604946612361,"user_tz":-60,"elapsed":866788,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"18590347-0d86-4059-ac9f-318652b68aa9","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Function definition\n","\n","def cosine_similarity(p, q, transpose_p=False, transpose_q=False):\n","    \"\"\"\n","    Computes the cosine similarity of two d-dimensional matrices\n","\n","    :param p: d-dimensional vector (np.ndarray) of shape (p_samples, d)\n","    :param q: d-dimensional vector (np.ndarray) of shape (q_samples, d)\n","    :param transpose_p: whether to transpose p or not\n","    :param transpose_q: whether to transpose q or not\n","\n","    :return\n","        - cosine similarity matrix S of shape (p_samples, q_samples)\n","          where S[i, j] = s(p[i], q[j])\n","    \"\"\"\n","    \n","    # If it is a vector, consider it as a single sample matrix\n","    if (p==q).all() and (p.shape==q.T.shape):\n","      from sklearn.metrics import pairwise_distances\n","      sim = 1 - pairwise_distances(p)\n","\n","    elif not (p.shape==q.T.shape):\n","      mat = np.dot(p,q.T)\n","      d = np.diag(mat)\n","      inv_diag = 1/d\n","      inv_diag = np.sqrt(inv_diag)\n","      c = mat*inv_diag\n","      sim = c.T*inv_diag\n","    else:\n","      from scipy.spatial import distance\n","      if len(p.shape) == 1:\n","        p = p.reshape(1, -1)\n","      if len(q.shape) == 1:\n","        q = q.reshape(1, -1)\n","      sim = 1 - distance.cdist(p,q, metric='cosine')\n","    return sim\n","\n","\n","# Testing\n","\n","print(\"Computing similarity matrix...\")\n","similarity_matrix = cosine_similarity(co_occurrence_matrix,\n","                                      co_occurrence_matrix)\n","print(\"Similarity completed!\")\n","\n","# Evaluation\n","\n","def evaluate_cosine_similarity(similarity_matrix):\n","\n","    # Vector similarity\n","    print('[Cosine similarity Evaluation] Vector similarity check...')\n","\n","    p = np.array([5., 6., 0.3, 1.])\n","    q = np.array([50., 6., 0., 0.])\n","    assert np.allclose(np.array([[0.72074324]]), manually_computed_consine_similarity(p, q))\n","\n","    # Matrix similarity\n","    print('[Cosine similarity Evaluation] Matrix similarity check...')\n","\n","    toy_matrix = np.array([5., 6., 0.3, 1.,\n","                           50., 6., 0., 0.,\n","                           0., 100., 20., 4.]).reshape(3, 4)\n","    true_matrix = np.array([1., 0.72074324, 0.75852259,\n","                            0.72074324, 1., 0.11674173,\n","                            0.75852259, 0.11674173, 1.]).reshape(3, 3)\n","    proposed_matrix = manually_computed_consine_similarity(toy_matrix, toy_matrix)\n","    \n","    assert np.allclose(proposed_matrix, true_matrix)\n","    assert np.equal(proposed_matrix, proposed_matrix.transpose()).all()\n","\n","    assert np.equal(similarity_matrix, similarity_matrix.transpose()).all()\n","\n","print('Evaluating cosine similarity...')\n","evaluate_cosine_similarity(similarity_matrix)\n","print('Evaluation completed!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Computing similarity matrix...\n","Similarity completed!\n","Evaluating cosine similarity...\n","[Cosine similarity Evaluation] Vector similarity check...\n","[Cosine similarity Evaluation] Matrix similarity check...\n","Evaluation completed!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JPnB9NrHETC8"},"source":["#### **[Let's play!] Synonyms and Antonyms**\n","\n","Look for some words and provide a possible explanation of achieved results according to cosine similarity metric (you can also refer to previous visualization step).\n","\n","* Synonym pair: (w1, w2) such that w1 and w2 are synonyms\n","* Antonyms pair: (w1, w2) such that w1 and w2 are antonyms\n","* Synonym-Antonym triplet: (w1, w2, w3) such that w1 and w2 are synonyms and w1 and w3 are antonyms\n","\n","You can also support your answer by checking word contexts of selected words."]},{"cell_type":"code","metadata":{"id":"3UvgENf2FsO5"},"source":["### YOUR CODE HERE ###\n","def pair_synonyms(w1,w2,co_occurrence_matrix,thr=0.8):\n","    wv1 = co_occurrence_matrix[word_to_idx[w1]]\n","    wv2 = co_occurrence_matrix[word_to_idx[w2]]\n","    print(f\"Synonyms Words with cosine value {manually_computed_consine_similarity(wv1,wv2)}\" \n","          if manually_computed_consine_similarity(wv1,wv2)>=thr else f\"Not Synonyms with cosine value: {manually_computed_consine_similarity(wv1,wv2)}\" )\n","    return True if manually_computed_consine_similarity(wv1,wv2)>=thr else False\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-wH6Ez5zcyj","executionInfo":{"status":"ok","timestamp":1604946612368,"user_tz":-60,"elapsed":866781,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"51c4fdab-e4fb-485a-edbb-c5f8f21f1f0e","colab":{"base_uri":"https://localhost:8080/"}},"source":["pair_synonyms(\"good\",\"first\",co_occurrence_matrix,0.7);"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Synonyms Words with cosine value [[0.7178185]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O9_C_2He-kVE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWg4rqjCzf1D"},"source":["def pair_antonyms(w1,w2,co_occurrence_matrix,thr=0.2):\n","    wv1 = co_occurrence_matrix[word_to_idx[w1]]\n","    wv2 = co_occurrence_matrix[word_to_idx[w2]]\n","    print(f\"Antonyms Words with cosine value {manually_computed_consine_similarity(wv1,wv2)}\" \n","          if manually_computed_consine_similarity(wv1,wv2)<=thr else \n","          f\"Not antonyms with cosine value: {manually_computed_consine_similarity(wv1,wv2)}\" )\n","    return True if manually_computed_consine_similarity(wv1,wv2)<=thr else False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4ZdHWmvzi9-","executionInfo":{"status":"ok","timestamp":1604946612376,"user_tz":-60,"elapsed":866772,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"7b1c8742-6f51-4496-fb7c-eefcbaff6adf","colab":{"base_uri":"https://localhost:8080/"}},"source":["pair_antonyms(\"king\",\"prince\",co_occurrence_matrix);"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Antonyms Words with cosine value [[0.09740795]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3PN48AwSzl9u"},"source":["def syn_ant_triplet(w1,w2,w3, co_occurrence_matrix,thr=0.7):\n","    wv1 = co_occurrence_matrix[word_to_idx[w1]]\n","    wv2 = co_occurrence_matrix[word_to_idx[w2]]\n","    wv3 = co_occurrence_matrix[word_to_idx[w3]]\n","    print(f\"{w1} and {w2} are synonyms\" if manually_computed_consine_similarity(wv1,wv2)>=thr else \n","          f\"{w1} and {w2} are not synonyms\")\n","    print(f\"{w1} and {w3} are antonyms\" if manually_computed_consine_similarity(wv1,wv2)>=(1-thr) else \n","          f\"{w1} and {w3} are not antonyms\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgkXs9bnzm02","executionInfo":{"status":"ok","timestamp":1604946612381,"user_tz":-60,"elapsed":866761,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"cedf2a60-611b-4ddd-e7ce-2a3bc69cb898","colab":{"base_uri":"https://localhost:8080/"}},"source":["syn_ant_triplet(\"good\",\"first\",\"bad\",co_occurrence_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["good and first are synonyms\n","good and bad are antonyms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hu38-5ZmFvMg"},"source":["** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n","I would say that for synonyms, for words with similar vector representations are likely to have high similarity measure.\n","And pair of words with very small similarity measure a are said to be antonym. This means that, one of the word is one cluster and the the other one is in another with with pairwise cosine similarity of the two clusters. However in my expereice i have notice that words synonyms computed with co_occurrence_matrix is efficient. so we get inexpected results. For some reasons, the word king and prince are antonymns since they far apart on the vcetor space"]},{"cell_type":"markdown","metadata":{"id":"mM5e-2kXF4XI"},"source":["#### **[Let's play!] Analogies**\n","\n","Another useful property to check is analogy resolution via word vectors. In particular, we might want to check if analogies such \"man : king == woman : x\" bring results like \"x = queen\".\n","\n","In order to do so, we first need to define a ranking function that returns the top $K$ most similar words of a given word vector. We might not want to be too much restrctive and play with $K \\ge 1$."]},{"cell_type":"code","metadata":{"id":"UJpe48ordWB8"},"source":["def get_top_K_indexes(data, K):\n","    \"\"\"\n","    Returns the top K indexes of a 1-dimensional array (descending order)\n","    Example:\n","        data = [0, 7, 2, 1]\n","        best_indexes:\n","        K = 1 -> [1] (data[1] = 7)\n","        K = 2 -> [1, 2]\n","        K = 3 -> [1, 2, 3]\n","        K = 4 -> [1, 2, 3, 4]\n","\n","    :param data: 1-d dimensional array\n","    :param K: number of highest value elements to consider\n","\n","    :return\n","        - array of indexes corresponding to elements of highest value\n","    \"\"\"\n","    best_indexes = np.argsort(data, axis=0)[::-1]\n","    best_indexes = best_indexes[:K]\n","\n","    return best_indexes\n","\n","def get_top_K_word_ranking(embedding_matrix, idx_to_word, word_to_idx,\n","                           positive_listing, negative_listing, K):\n","    \"\"\"\n","    Finds the top K most similar words following this reasoning:\n","        1. words that have highest similarity to words in positive_listing\n","        2. words that have highest distance to words in negative_listing\n","    \n","    Positive and negative listing can be defined accordingly to a given analogy\n","    Example:\n","        \n","        man : king :: woman : x\n","    \n","    positive_listing = ['king', 'woman']\n","    negative_listing = ['man']\n","\n","    This is equivalent to: compute king - man + woman, and then find the\n","    most similar candidate.\n","    \n","    :param embedding_matrix: embedding matrix of shape (words, embedding dimension).\n","    Note that in the case of a co-occurrence matrix, the shape is (words, words).\n","    :param idx_to_word: vocabulary map (index -> word) (dict)\n","    :param word_to_idx: vocabulary map (word -> index) (dict)\n","    :param positive_listing: list of words that should have high similarity with\n","                             top K retrieved ones.\n","    :param negative_listing: list of words that should have high distance to\n","                             top K retrieved ones.\n","    :param K: number of best word matches to consider\n","\n","    :return\n","        - top K word matches according to aforementioned criterium\n","        - similarity values of top K word matches according to aforementioned\n","          criterium\n","    \"\"\"\n","\n","\n","    # Positive words (similarity)\n","    positive_indexes = np.array([word_to_idx[word] for word in positive_listing])\n","    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n","\n","    # Negative words (distance)\n","    negative_indexes = np.array([word_to_idx[word] for word in negative_listing])\n","    word_negative_vector = np.sum(embedding_matrix[negative_indexes, :], axis=0)\n","\n","    # Find candidate words\n","    target_vector = (word_positive_vector - word_negative_vector) / (len(positive_listing) + len(negative_listing))\n","    total_indexes = np.concatenate((positive_indexes, negative_indexes))\n","    valid_indexes = np.setdiff1d(np.arange(similarity_matrix.shape[0]), total_indexes)\n","    candidate_vectors = embedding_matrix[valid_indexes]\n","\n","    candidate_similarities = manually_computed_consine_similarity(candidate_vectors, target_vector)\n","    candidate_similarities = candidate_similarities.ravel()\n","\n","    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n","    top_K_indexes = valid_indexes[relative_indexes]\n","    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n","\n","    return top_K_words, candidate_similarities[relative_indexes]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5V4c-mOpNem7"},"source":["Now do it yourself! Find some examples of analogies that hold and other that do not. Remember to give a proper explanation concerning obtained results.\n","\n","**Note**: 1-2 examples are sufficient. This exercies is just another way to inspect word embeddings."]},{"cell_type":"code","metadata":{"id":"vva-_bylNxIE","executionInfo":{"status":"ok","timestamp":1604946631051,"user_tz":-60,"elapsed":1627,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"95c12e1e-2499-4991-fa05-319d3c612734","colab":{"base_uri":"https://localhost:8080/"}},"source":["### MODIFY THIS ###\n","K = 10\n","\n","# Example analogy: tv : episodes :: film : x\n","# positive listing -> [episodes, film]\n","# negative listing ->  [tv]\n","# masterpiece : superb :: x : tragic\n","top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n","                                                   idx_to_word,\n","                                                   word_to_idx,\n","                                                   ['good', 'scenes'],\n","                                                   ['tragic'],\n","                                                   K)\n","print('Top K words: ', top_K_words)\n","print('Top K values: ', top_K_values)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top K words:  ['br', 'really', 'one', 'movie', 'film', 'like', 'well', 'see', 'plot', 'bad']\n","Top K values:  [0.83417404 0.81843078 0.80447865 0.80311894 0.79474217 0.78795177\n"," 0.77353352 0.76511717 0.75865281 0.75305259]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D2jJPJHbN3hR"},"source":["** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n","the following analogy vector(good)-vector(tragic)+vector(scenes) is almost = vector(br),second best vector is vector(really) and so on"]},{"cell_type":"markdown","metadata":{"id":"AxGPC1CC7mYn"},"source":["#### **[Let's play!] Bias**\n","\n","When we talk about societies we are usually aware of their related biases (gender, race, sexual orientation, etc..). Indeed, this fact is reflected at textual level. For example, when we consider the word 'doctor' we usually think of a 'man', whereas when we consider the word 'nurse' we usually think of a 'woman'.\n","\n","Let's see if word embeddings reflect such harmful biases. Find an example of bias by following the analogy approach."]},{"cell_type":"code","metadata":{"id":"dkuKPZsJUPgH","executionInfo":{"status":"ok","timestamp":1604946635130,"user_tz":-60,"elapsed":1710,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"ddedccef-f061-40cf-b441-18b4e714f40c","colab":{"base_uri":"https://localhost:8080/"}},"source":["### YOUR CODE HERE ###\n","top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix,\n","                                                   idx_to_word,\n","                                                   word_to_idx,\n","                                                   [\"doctor\"],\n","                                                   ['nurse'],\n","                                                   K)\n","print('Top K words: ', top_K_words)\n","print('Top K values: ', top_K_values)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top K words:  ['xrayed', 'aforementioned', 'ohyeah', 'nurses', 'exhibiting', 'nightstick', 'spaceship', 'swollen', 'cheetah', 'swings']\n","Top K values:  [0.23179317 0.19867986 0.19867986 0.19867986 0.16556655 0.16556655\n"," 0.16556655 0.16556655 0.15452878 0.14808722]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"twScPzVqUSmt"},"source":["** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n","the word doctor and nurse reflect somehow the cutural biases related to the field of hospital such as xrayed, nurses, swings and so on\n"]},{"cell_type":"markdown","metadata":{"id":"Y6cyDMwtLWQ4"},"source":["## Better sparse word embeddings\n","\n","Until now we've played with the most basic type of word encoding, that is count-based co-occurrence matrix. However, there are better ways to encode words.\n","\n","In particular, we will explore positive pointwise mutual information (PPMI) weighting technique."]},{"cell_type":"markdown","metadata":{"id":"tvsS4nY6LjOm"},"source":["### PPMI\n","\n","Pointwise mutual information (PMI) is a weighting technique, just like tf-dif, that gives more weight to word pairs based on how often they occur within the same context window that we would have expected them to appear by chance.\n","\n","$PMI(w1, w2) = \\log_2 \\frac{P(w1, w2)}{P(w1)P(w2)}$\n","\n","PMI value range is $[-\\infty, \\infty]$, but negative values are a bit tricky, unless we have a very big corpus. Thus, it is more common to replace all negative PMI values with zero.\n","\n","$PPMI(w1, w2) = \\max(\\log_2 \\frac{P(w1, w2)}{P(w1)P(w2)}, 0)$\n","\n","Now, it's your turn to weight the count-based co-occurrence matrix with PPMI technique.\n","\n","**PPMI Memo**: \n","Given a co-occurrence matrix C of shape (N, M), we can turn it into a PPMI matrix as follows:\n","\n","$p_{i,j} = \\frac{C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n","\n","$p_{i,*} = \\frac{\\sum_{j=1}^M C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n","\n","$p_{*,j} = \\frac{\\sum_{i=1}^N C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n","\n","$PPMI_{i, j} = \\max(\\log_2 \\frac{p_{i, j}}{p_{i, *} \\, p_{*, j}}, 0)$"]},{"cell_type":"code","metadata":{"id":"Qre__i5qcIt7"},"source":["def convert_ppmi(co_occurrence_matrix):\n","    \"\"\"\n","    Converts a count-based co-occurrence matrix to a PPMI matrix\n","\n","    :param co_occurrence_matrix: count based co-occurrence matrix of shape (|V|, |V|)\n","    \n","    :return\n","        - PPMI co-occurrence matrix of shape (|V|, |V|)\n","    \"\"\"\n","\n","    ### YOUR CODE HERE ###\n","    \n","    def P_ij(m):\n","      s = np.sum(m)\n","      return m/s\n","\n","    \n","    def P_i(m):\n","      s = np.sum(m)\n","      s_i = np.sum(m,axis=1)\n","      return s_i/s\n","    \n","    def P_j(m):\n","      s = np.sum(m)\n","      s_j = np.sum(m,axis=0)\n","      return s_j/s\n","\n","    def PMI(m):\n","      p1 = P_i(m).reshape(1,-1)\n","      p2 = P_j(m).reshape(1,-1)   \n","      return np.log2(P_ij(m)/np.dot(p1.T,p2))\n","\n","    \n","    def PPMI(m):\n","      p = np.zeros_like(PMI(m))\n","      for i in range(p.shape[0]):\n","        for j in range(p.shape[1]):\n","          p[i,j] = max(PMI(m)[i,j],0)\n","      return p\n","    return PPMI(co_occurrence_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyf7vHgPcRAk","executionInfo":{"status":"ok","timestamp":1604946647873,"user_tz":-60,"elapsed":914,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"4a3d8967-40ac-42ab-f921-aa99b4855c12","colab":{"base_uri":"https://localhost:8080/"}},"source":["# We should already have download co-occurrence benchmark data\n","benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n","toy_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark')\n","toy_idx_to_word = np.load(os.path.join(toy_path, 'toy_idx_to_word.npy'), allow_pickle=True).item()\n","toy_word_to_idx = np.load(os.path.join(toy_path, 'toy_word_to_idx.npy'), allow_pickle=True).item()\n","toy_valid_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_count.npy'))\n","\n","toy_ppmi_matrix = convert_ppmi(toy_valid_matrix)\n","toy_valid_ppmi_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_ppmi.npy'))\n","\n","# checking\n","assert np.equal(toy_ppmi_matrix, toy_valid_ppmi_matrix).all()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in log2\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8DcQJ1dvmCY2"},"source":["\n","# Function definition\n","\n","def convert_ppmi(co_occurrence_matrix):\n","    \"\"\"\n","    Converts a count-based co-occurrence matrix to a PPMI matrix\n","\n","    :param co_occurrence_matrix: count based co-occurrence matrix of shape (|V|, |V|)\n","    \n","    :return\n","        - PPMI co-occurrence matrix of shape (|V|, |V|)\n","    \"\"\"\n","\n","    ### YOUR CODE HERE ###\n","    \n","    def P_ij(m):\n","      s = np.sum(m)\n","      return m/s\n","\n","    \n","    def P_i(m):\n","      s = np.sum(m)\n","      s_i = np.sum(m,axis=1)\n","      return s_i/s\n","    \n","    def P_j(m):\n","      s = np.sum(m)\n","      s_j = np.sum(m,axis=0)\n","      return s_j/s\n","\n","    def PMI(m):\n","      p1 = P_i(m).reshape(1,-1)\n","      p2 = P_j(m).reshape(1,-1)   \n","      return np.log2(P_ij(m)/np.dot(p1.T,p2))\n","\n","    \n","    def PPMI(m):\n","      p = np.zeros_like(PMI(m))\n","      for i in range(p.shape[0]):\n","        for j in range(p.shape[1]):\n","          p[i,j] = max(PMI(m)[i,j],0)\n","      return p\n","    return PPMI(co_occurrence_matrix)\n","\n","# Testing\n","\n","print(\"Computing PPMI co-occurrence matrix...\")\n","ppmi_occurrence_matrix = convert_ppmi(co_occurrence_matrix)\n","print(\"PPMI completed!\")\n","\n","# Evaluation\n","\n","def evaluate_ppmi_matrix(matrix):\n","    is_sparse = False\n","\n","    if hasattr(scipy.sparse, type(matrix).__name__):\n","        print(\"Detected sparse PPMI co-occurrence matrix!\")\n","        is_sparse = True\n","\n","    # Check symmetry\n","    print(\"[Co-occurrence PPMI matrix Evaluation] Symmetry checking...\")\n","    if is_sparse:\n","        try:\n","            assert (matrix != matrix.transpose()).nnz == 0\n","        except AssertionError:\n","            assert sparse_allclose(matrix, matrix.transpose())\n","    else:\n","        try:\n","            assert np.equal(matrix, matrix.transpose()).all()\n","        except AssertionError:\n","            assert np.allclose(matrix, matrix.transpose())\n","\n","    # A very simple example\n","    print(\"[Co-occurrence PPMI matrix Evaluation] Toy example checking...\")\n","\n","    toy_df = pd.DataFrame.from_dict({\n","        'sentence_1': [\"All that glitters is not gold\"],\n","        'sentence_2': [\"All in all I like this assignment\"]\n","    })\n","\n","    # We should already have download co-occurrence benchmark data\n","    benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n","    toy_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark')\n","    toy_idx_to_word = np.load(os.path.join(toy_path, 'toy_idx_to_word.npy'), allow_pickle=True).item()\n","    toy_word_to_idx = np.load(os.path.join(toy_path, 'toy_word_to_idx.npy'), allow_pickle=True).item()\n","    toy_valid_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_count.npy'))\n","\n","    toy_ppmi_matrix = convert_ppmi(toy_valid_matrix)\n","    toy_valid_ppmi_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_ppmi.npy'))\n","\n","    if is_sparse:\n","        try:\n","            assert (toy_ppmi_matrix != toy_valid_ppmi_matrix).nnz == 0\n","        except AssertionError:\n","            assert sparse_allclose(toy_ppmi_matrix, toy_valid_ppmi_matrix)\n","    else:\n","        try:\n","            assert np.equal(toy_ppmi_matrix, toy_valid_ppmi_matrix).all()\n","        except AssertionError:\n","            assert np.allclose(toy_ppmi_matrix, toy_valid_ppmi_matrix)\n","\n","\n","print('Evaluating PPMi matrix conversion...')\n","evaluate_ppmi_matrix(ppmi_occurrence_matrix)\n","print('Evaluation completed!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JQwP1ee1_NPV"},"source":["### **Got Stuck?**\n","\n","If you are stuck, but still want to try out following sections, you can experiment with a valid PPMI co-occurrence matrix provided by us as follows"]},{"cell_type":"code","metadata":{"id":"EdRRGT6Z_QSE","executionInfo":{"status":"ok","timestamp":1604946655164,"user_tz":-60,"elapsed":780,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"40dacd88-af05-4a51-e36c-35a2716e3ba1","colab":{"base_uri":"https://localhost:8080/"}},"source":["benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n","valid_data_benchmark_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark', \"{}.npy\")\n","\n","ppmi_occurrence_matrix = np.load(valid_data_benchmark_path.format('valid_co-occurrence_matrix_ppmi'))\n","\n","print('PPMI Co-occurrence matrix shape: ', ppmi_occurrence_matrix.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PPMI Co-occurrence matrix shape:  (14244, 14244)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lQOx7ZtLyJTn"},"source":["### Visualization (cont'd)\n","\n","Let's see if these weighting techniques have brought some change at visualization level!\n","\n","Pick a dimensionality reduction technique and explore the new embedding space."]},{"cell_type":"code","metadata":{"id":"sa2E6yWDyb39"},"source":["### YOUR CODE HERE ###\n","# Note: this method may take a while (just relax :-))\n","reduced_tSNE = reduce_tSNE(ppmi_occurrence_matrix)\n","visualize_embeddings(reduced_tSNE, ['good', 'love', 'beautiful'], word_to_idx)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7Ud--aA8Jlw"},"source":["Feel free to play with visualization!"]},{"cell_type":"code","metadata":{"id":"AvyDUtKM8LXf"},"source":["### YOUR CODE HERE ###\n","# SVD\n","reduced_SVD = reduce_SVD(ppmi_occurrence_matrix)\n","visualize_embeddings(reduced_SVD, ['bad', 'first', 'king', 'prince','good', 'hello', 'yeah'], word_to_idx)\n","\n","# t-SNE\n","# Note: this method may take a while (just relax :-))\n","reduced_tSNE = reduce_tSNE(ppmi_occurrence_matrix)\n","visualize_embeddings(reduced_tSNE,['bad', 'first', 'king', 'prince','good', 'hello', 'yeah'], word_to_idx)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZY70qU-53rFy"},"source":["### [Let's play!] Embedding properties (cont'd)\n","\n","Choose a word embedding property to analyse (synonyms, analogies, bias) and either select and describe a new example or pick an old one and compare previously achieved results with current ones!"]},{"cell_type":"code","metadata":{"id":"qGvEcMrs4Yvq"},"source":["### YOUR CODE HERE  ###\n","\n","# from the above visualization, we can say that words good and first are pretty similar due their spatial proximity\n","# words such as good and bad are far away from each other."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eryrXsKS1D25","executionInfo":{"status":"ok","timestamp":1604947160952,"user_tz":-60,"elapsed":2522,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"4f711598-c4fb-4f14-9437-c57c4cf51a0b","colab":{"base_uri":"https://localhost:8080/"}},"source":["### YOUR CODE HERE ### Bias Analysis\n","top_K_words, top_K_values = get_top_K_word_ranking(ppmi_occurrence_matrix,\n","                                                   idx_to_word,\n","                                                   word_to_idx,\n","                                                   [\"doctor\"],\n","                                                   ['nurse'],\n","                                                   K)\n","print('Top K words: ', top_K_words)\n","print('Top K values: ', top_K_values)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top K words:  ['aforementioned', 'xrayed', 'import', 'nightstick', 'behaves', 'daughterlove', 'nurses', 'stargate', 'rabid', 'dejavus']\n","Top K values:  [0.22522923 0.21203099 0.20985842 0.20377153 0.19180827 0.18680747\n"," 0.18163694 0.1794219  0.1783333  0.17408486]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tc4YevLsx3xY"},"source":["# [Part II] Dense embeddings\n","\n","Until now we've worked with sparse embedding methods, which lead to high dimensional word embeddings (dimension equal to |V|). The main drawback of such approach is that words belong to separate dimensions. Thus, in order to check if two words have similar contexts we need to have a large corpus available.\n","\n","To this end, we might prefer a dense embedding technique, such that all words are encoded to high dimensional space, much smaller than |V| (generally up to $\\sim$ 1000). A dense representation is also convenient from a machine learning point of view: we have fewer parameters to learn and, thus, models are less prone to overfitting. Moreover, words do not belong to separate dimensions anymore and semantic relationships are easily modelled.\n","\n","In this section, we will experiment with pre-trained dense embedding models and compare them to previously described sparse methods."]},{"cell_type":"markdown","metadata":{"id":"4i_gxgZK5VGM"},"source":["## Working with a pre-trained model\n","\n","The first step consists in choosing and downloading a pre-trained embedding model. For the purpose of this assignment, we limit to classic models, such as Word2Vec and GloVe.\n","\n","Furthermore, some pre-trained embedding model versions may be quite resource demanding, depending on the embedding dimension and on the vocabulary size. We recommend sticking to low dimensional spaces (50, 100, 200) to avoid being stuck waiting for too much time."]},{"cell_type":"markdown","metadata":{"id":"zoU1fqAs5XxI"},"source":["### Download embedding model\n","\n","Downloading a pre-trained embedding model is quite simple to due existing ad hoc wrappers. In particular, we will use [Gensim](https://radimrehurek.com/gensim/) library for both embedding models as follows."]},{"cell_type":"code","metadata":{"id":"LV0RQIFT-Sd3","executionInfo":{"status":"ok","timestamp":1604947526460,"user_tz":-60,"elapsed":44296,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"8c2aac37-9d56-4a22-c750-a8eb9779dc31","colab":{"base_uri":"https://localhost:8080/"}},"source":["import gensim\n","import gensim.downloader as gloader\n","\n","def load_embedding_model(model_type, embedding_dimension=50):\n","    \"\"\"\n","    Loads a pre-trained word embedding model via gensim library.\n","\n","    :param model_type: name of the word embedding model to load.\n","    :param embedding_dimension: size of the embedding space to consider\n","\n","    :return\n","        - pre-trained word embedding model (gensim KeyedVectors object)\n","    \"\"\"\n","\n","    download_path = \"\"\n","\n","    # Find the correct embedding model name\n","    if model_type.strip().lower() == 'word2vec':\n","        download_path = \"word2vec-google-news-300\"\n","\n","    elif model_type.strip().lower() == 'glove':\n","        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n","\n","    else:\n","        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\n","\n","    # Check download\n","    try:\n","        emb_model = gloader.load(download_path)\n","    except ValueError as e:\n","        print(\"Invalid embedding model name! Check the embedding dimension:\")\n","        print(\"Word2Vec: 300\")\n","        print(\"Glove: 50, 100, 200, 300\")\n","        raise e\n","\n","    return emb_model\n","\n","\n","# Modify these variables as you wish!\n","# Glove -> 50, 100, 200, 300\n","# Word2Vec -> 300\n","embedding_model_type = \"glove\"\n","embedding_dimension = 50\n","\n","embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)\n","        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AEL3Hrjl5dfs"},"source":["### Out of vocabulary (OOV) words\n","\n","Before evaluating pre-trained dense word embeddings, it is good practice to check if the model is consistent with our dataset. To do so, we check the number of out-of-vocabulary (OOV) terms.\n","\n","If the OOV amount is negligible, we can just keep going. On the other hand, we might want to handle OOV terms by assigning them a specific word vector.\n","\n","**Which one?** One common practice is to assign a random vector, since the embedding model will be part of a deep learning model and, thus, word vectors might be trained during the learning process. Even if that is the case, we can assign an embedding that is more meaningful rather than a random one: for example, we can identify the word embedding of an OOV term as the mean of its neighbour word embeddings.\n","\n","Check out OOV terms and assign a meaningful word embedding. Then, at the visualization step, check if this strategy reflects words semantic properties."]},{"cell_type":"code","metadata":{"id":"k_L_ewVPH7VQ","executionInfo":{"status":"ok","timestamp":1604947526471,"user_tz":-60,"elapsed":38936,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"c724c67c-6637-4984-fc55-5d476efbd72f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Function definition\n","\n","def check_OOV_terms(embedding_model, word_listing):\n","    \"\"\"\n","    Checks differences between pre-trained embedding model vocabulary\n","    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n","\n","    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n","    :param word_listing: dataset specific vocabulary (list)\n","\n","    :return\n","        - list of OOV terms\n","    \"\"\"\n","\n","    ### YOUR CODE HERE ###\n","    \n","    return [w for w in word_listing if not w in embedding_model.vocab]\n","\n","\n","oov_terms = check_OOV_terms(embedding_model, word_listing)\n","\n","print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(word_listing)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total OOV terms: 1941 (0.14%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bfzU7KGwnEe1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjNoxZb--5O-"},"source":["### Handling OOV words\n","\n","Now we proceed on building the embedding matrix, while handling OOV terms at the same time. \n","\n","Experiment with/without the OOV custom encoding strategy.\n","\n","**NOTE**: Here we ask you to implement both OOV strategies! Feel free to either write two separate functions or modify the given function signature."]},{"cell_type":"code","metadata":{"id":"XC1F_44lJUDF","executionInfo":{"status":"ok","timestamp":1604947840602,"user_tz":-60,"elapsed":932,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"00359799-89df-4867-987d-cf09a70a5242","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Function definition\n","\n","def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, oov_terms, co_occurrence_count_matrix):\n","    \"\"\"\n","    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n","\n","    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n","    :param word_to_idx: vocabulary map (word -> index) (dict)\n","    :param oov_terms: list of OOV terms (list)\n","    :param co_occorruence_count_matrix: the co-occurrence count matrix of the given dataset (window size 1)\n","\n","    :return\n","        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n","    \"\"\"\n","\n","    ### YOUR CODE HERE ###\n","    import copy\n","\n","    mat = np.zeros((len(word_to_idx)-len(oov_terms),embedding_dimension), dtype=np.float32)\n","    copy_word = copy.copy(word_to_idx)\n","\n","    d = {copy_word.pop(m) for m in oov_terms}\n","\n","    for i,item in enumerate(copy_word.items()):\n","      if not item[0] in embedding_model.vocab.keys():\n","        pass\n","      else:\n","          mat[i] = embedding_model[item[0]]\n","\n","    return mat\n","\n","\n","# Testing\n","\n","embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension,word_to_idx,oov_terms,None)\n","\n","print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Embedding matrix shape: (12303, 50)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N8p1-GQQB0Vh"},"source":["from functools import reduce\n","k = 10\n","\n","def Gen_sim_word(embedding_model,word_listing):\n","    sim_words,_ = get_top_K_word_ranking_of_oov(co_occurrence_matrix,idx_to_word,word_to_idx,[word_listing[0]],k)\n","    sim_words = [w for w in sim_words if not w in oov_terms]\n","    avg = reduce(lambda a,b: a+b ,[embedding_model[w] for w in sim_words])\n","    \n","    return avg/len(sim_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMiWbNraCIhg"},"source":["def get_top_K_word_ranking_of_oov(embedding_matrix, idx_to_word, word_to_idx,\n","                           word_listing, K):\n","    \n","    # Positive words (similarity)\n","    positive_indexes = np.array([word_to_idx[word] for word in word_listing])\n","    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n","\n","    # Find candidate words\n","    target_vector = word_positive_vector / (len(word_listing))\n","    total_indexes = positive_indexes\n","    valid_indexes = np.setdiff1d(np.arange(similarity_matrix.shape[0]), total_indexes)\n","    candidate_vectors = embedding_matrix[valid_indexes]\n","\n","    candidate_similarities = manually_computed_consine_similarity(candidate_vectors, target_vector)\n","    candidate_similarities = candidate_similarities.ravel()\n","\n","    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n","    top_K_indexes = valid_indexes[relative_indexes]\n","    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n","\n","    return top_K_words, candidate_similarities[relative_indexes]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOMEO5hLHiQn"},"source":["%%time\n","def build_embedding_matrix_with(embedding_model, embedding_dimension, word_to_idx, oov_terms):\n","    \"\"\"\n","    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n","\n","    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n","    :param word_to_idx: vocabulary map (word -> index) (dict)\n","    :param oov_terms: list of OOV terms (list)\n","    :param co_occorruence_count_matrix: the co-occurrence count matrix of the given dataset (window size 1)\n","\n","    :return\n","        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n","    \"\"\"\n","\n","    ### YOUR CODE HERE ###\n","    \n","    mat = np.zeros((len(word_to_idx),embedding_dimension), dtype=np.float32)\n","    copy_word = word_to_idx.copy()\n","\n","    for i,item in enumerate(copy_word.items()):\n","        if not item[0] in embedding_model.vocab.keys():\n","            mat[i] = Gen_sim_word(embedding_model,[item[0]])\n","        else:\n","            mat[i] = embedding_model[item[0]]\n","    return mat\n","\n","\n","# Testing\n","\n","embedding_matrix_of_oov = build_embedding_matrix_with(embedding_model, embedding_dimension,word_to_idx,oov_terms)\n","#embedding_matrix_of_oov = np.load(\"embedding_matrix_with_oov_terms.npy\")\n","\n","print(\"Embedding matrix shape: {}\".format(embedding_matrix_of_oov.shape))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ghxH5floBzd"},"source":["embedding_matrix_of_oov[word_to_idx[oov_terms[9]]] # before it was vector full of zeros"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hirKvw8x5kd4"},"source":["## Embedding visualization (cont'd)\n","\n","We are now ready to visualize pre-trained word embeddings!"]},{"cell_type":"code","metadata":{"id":"ccE5hxlARz4X"},"source":["### YOUR CODE HERE ###\n","\n","# SVD\n","reduced_SVD = reduce_SVD(embedding_matrix)\n","visualize_embeddings(reduced_SVD, ['good', 'love', 'beautiful'], word_to_idx)\n","\n","# SVD\n","#reduced_SVD = reduce_SVD(embedding_matrix_of_oov)\n","#visualize_embeddings(reduced_SVD, ['good', 'love', 'beautiful'], word_to_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEkhU2Qjz45k"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AApBQmfC8STB"},"source":["Feel free to play with visualization!"]},{"cell_type":"code","metadata":{"id":"dZU-LQSo8UIh"},"source":["### YOUR CODE HERE ###\n","# SVD\n","reduced_SVD = reduce_SVD(ppmi_occurrence_matrix)\n","visualize_embeddings(reduced_SVD, ['bad', 'first', 'king', 'prince','good'], word_to_idx)\n","\n","# t-SNE\n","# Note: this method may take a while (just relax :-))\n","#reduced_tSNE = reduce_tSNE(ppmi_occurrence_matrix)\n","#visualize_embeddings(reduced_tSNE,['bad', 'first', 'king', 'prince','good'], word_to_idx)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MxVC_fOo5oqQ"},"source":["## [Let's play!] Embedding properties (cont'd)\n","\n","Choose a word embedding property to analyse (synonyms, analogies, bias) and either select and describe a new example or pick an old one and compare previously achieved results with current ones!"]},{"cell_type":"markdown","metadata":{"id":"1F6kSn7DYNBw"},"source":["The following analysis is using embedding_matrix without oov"]},{"cell_type":"code","metadata":{"id":"C_9UXGa-bNIB"},"source":["def generate_syn(word, embedding_matrix=embedding_matrix,thr=0.8,k=5):\n","    d = dict(zip([w for w in word_to_idx if not w in oov_terms],range(len(word_to_idx)-len(oov_terms))))\n","    r = [(x,manually_computed_consine_similarity(embedding_matrix[d[word]],\n","                                                             embedding_matrix[d[x]])) for x in [w for w in d if \n","                        manually_computed_consine_similarity(embedding_matrix[d[word]],\n","                                                             embedding_matrix[d[w]])>=thr] if x!=word]\n","    r1 = [x[1] for x in r]   \n","    best_index = get_top_K_indexes(r1,k)\n","    best_index = [x[0][0] for x in best_index]\n","    return [(r[x][0],r[x][1][0][0]) for x in best_index]                                     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qt8Q1hACDMNv","executionInfo":{"status":"ok","timestamp":1604947876754,"user_tz":-60,"elapsed":5861,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"d8ab31af-4eb6-4dee-c700-481b4ea7d7c5","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(generate_syn(\"first\",k=2))\n","print(generate_syn(\"great\",k=2))\n","print(generate_syn(\"good\",k=2))\n","print(generate_syn(\"love\",k=2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('second', 0.9318253397941589), ('third', 0.9012621641159058)]\n","[('greatest', 0.8162123560905457)]\n","[('better', 0.9284391403198242), ('really', 0.9220625162124634)]\n","[('dream', 0.8429608941078186), ('life', 0.8403438329696655)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DLVI_KzGEEWG"},"source":["def sim_oov_terms(similar_matrix, word_to_idx):\n","    mat = np.zeros((len(word_to_idx)-len(oov_terms),embedding_dimension), dtype=np.float32)\n","    copy_word = word_to_idx.copy()\n","\n","    d = {copy_word.pop(m) for m in oov_terms}\n","\n","    for i,item in enumerate(copy_word.items()):\n","      if not item[0] in embedding_model.vocab.keys():\n","        pass\n","      else:\n","          mat[i] = embedding_model[item[0]]\n","\n","    return mat\n","\n","similarity_matrix_oov = sim_oov_terms(similarity_matrix,word_to_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82Fjuqg9SJIO"},"source":["def get_top_K_word_ranking_oov(embedding_matrix, idx_to_word, word_to_idx,\n","                           positive_listing, negative_listing, K):\n","    \n","\n","    # Positive words (similarity)\n","    positive_indexes = np.array([word_to_idx[word] for word in positive_listing])\n","    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n","\n","    # Negative words (distance)\n","    negative_indexes = np.array([word_to_idx[word] for word in negative_listing])\n","    word_negative_vector = np.sum(embedding_matrix[negative_indexes, :], axis=0)\n","\n","    # Find candidate words\n","    target_vector = (word_positive_vector - word_negative_vector) / (len(positive_listing) + len(negative_listing))\n","    total_indexes = np.concatenate((positive_indexes, negative_indexes))\n","    valid_indexes = np.setdiff1d(np.arange(similarity_matrix_oov.shape[0]), total_indexes)\n","    candidate_vectors = embedding_matrix[valid_indexes]\n","\n","    candidate_similarities = manually_computed_consine_similarity(candidate_vectors, target_vector)\n","    candidate_similarities = candidate_similarities.ravel()\n","\n","    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n","    #relative_indexes = relative_indexes[::-1]\n","    top_K_indexes = valid_indexes[relative_indexes]\n","    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n","\n","    return top_K_words, candidate_similarities[relative_indexes]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qh0hYqS9THpB"},"source":["embedding_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsws9mzYEdWK","executionInfo":{"status":"ok","timestamp":1604947889913,"user_tz":-60,"elapsed":1462,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"d52e1491-f1ad-4d02-ed66-c5faaf9bb498","colab":{"base_uri":"https://localhost:8080/"}},"source":["### MODIFY THIS ### Analogies analysis without OOV\n","K = 10\n","\n","d = dict(zip([w for w in word_to_idx if not w in oov_terms],range(len(word_to_idx)-len(oov_terms))))\n","d0 = dict(zip(range(len(d)),d.keys()))\n","top_K_words, top_K_values = get_top_K_word_ranking_oov(embedding_matrix,\n","                                                   d0,\n","                                                   d,\n","                                                   ['man', 'woman'],\n","                                                   ['trouble'],\n","                                                   K)\n","print('Top K words: ', top_K_words)\n","print('Top K values: ', top_K_values)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top K words:  ['girl', 'boy', 'mother', 'old', 'wife', 'soldier', 'lover', 'daughter', 'haired', 'dies']\n","Top K values:  [0.82647306 0.81175041 0.77514434 0.74301505 0.72894847 0.72855252\n"," 0.7142325  0.71111006 0.7075749  0.70641994]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZUftzi-t2g06","executionInfo":{"status":"ok","timestamp":1604947893155,"user_tz":-60,"elapsed":1172,"user":{"displayName":"djalilou ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivCNwGPoIvRniGsUT0JoYiANxs0EwL4QmyZ5GFiw=s64","userId":"12017855888319370142"}},"outputId":"fa90c81a-18fc-4861-9c29-d31af90b4fe5","colab":{"base_uri":"https://localhost:8080/"}},"source":["### MODIFY THIS ###\n","K = 10\n","# BIAS\n","d = dict(zip([w for w in word_to_idx if not w in oov_terms],range(len(word_to_idx)-len(oov_terms))))\n","d0 = dict(zip(range(len(d)),d.keys()))\n","top_K_words, top_K_values = get_top_K_word_ranking_oov(embedding_matrix,\n","                                                   d0,\n","                                                   d,\n","                                                   ['man'],\n","                                                   ['woman'],\n","                                                   K)\n","print('Top K words: ', top_K_words[::-1])\n","print('Top K values: ', top_K_values[::-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top K words:  ['rockets', 'boss', 'prop', 'monty', 'ham', 'colletti', 'skipper', 'ravens', 'rangers', 'hammers']\n","Top K values:  [0.43985543 0.44397679 0.44493508 0.45009226 0.45670372 0.45860654\n"," 0.46335071 0.51281798 0.51486635 0.53528494]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SyPDNB56UhmM"},"source":["# Contact\n","\n","For any doubt, question, issue or help, you can always contact us at the following email addresses:\n","\n","Teaching Assistants:\n","\n","* Andrea Galassi -> a.galassi@unibo.it\n","* Federico Ruggeri -> federico.ruggeri6@unibo.it\n","\n","Professor:\n","\n","* Paolo Torroni -> p.torroni@unibo.it\n","\n","Don't forget that your feedback is very important! Your suggestions help us improving course material."]},{"cell_type":"markdown","metadata":{"id":"wNXyqKePNbth"},"source":["# FAQ\n","\n","---\n","\n","**Q: Is it ok if I work with a small slice of the dataset?**\n","\n","**A:** Yes, it is perfectly ok! The aim of this assignment is to look at word embedding methods and assess semantic properties. Large datasets usually imply large vocabularies and efficient sparse encoding methods have to be considered. Since such methods (see scipy documentation) might be complex to handle (especially under a colab session), you are free to work with small corpora.\n","\n","---\n","\n","**Q: Do I have to use both dimensionality reduction methods?**\n","\n","**A:** Just one is fine! We suggest to try both of them at least once!\n","\n","---\n","\n","**Q: I'm struggling find good examples for analogies, bias and other scenarios!**\n","\n","**A:** It is perfectly fine, this is just a simple example where we want you to look at\n","different encoding methods. Try to find at least one example that sounds good to you and motivate obtained results. Most probably, you won't find a perfect corrispondence of your expectations, but for us what's important is that you develop a critic approach, baring in mind that data has to explored before anything else.\n","\n","---\n","\n","**Q: Isn't stopwords removal excessive?**\n","\n","**A:** Indeed, removing all stopwords might alter sentence meaning! However, they also alter co-occurrence matrices due to their high frequency. Not ignoring them leads to poor results when considering semantic properties. It's up to you whether removing or keeping stopwords! (remember to comment the corresponding method under [Some Cleaning](https://colab.research.google.com/drive/1UkGz0vdhPXh9NeApG7mYtY6e-jRcR3if#scrollTo=2TLTu0-2JQwi&line=3&uniqifier=1) section.\n","\n","---\n","\n","**Q: Can we modify functions signature?**\n","\n","**A:** Functions that you have to complete can be modified as you please! Current functions definition should consider all required inputs in most cases.\n","\n","---\n","\n","**Q: Can we modify the dataset slicing step**\n","\n","**A:** Yes, of course! The current slicing is just one possibility.\n","\n","---\n"]}]}