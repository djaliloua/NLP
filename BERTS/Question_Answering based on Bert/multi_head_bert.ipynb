{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multi_head_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7sypFcvvZHX"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j68pPERtkE5v",
        "outputId": "24e09ab6-7f77-4664-c964-e61c9d6e57c4"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 5.0MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5tRwq9f5OIF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import json\n",
        "import collections\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import tokenizers\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import string \n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACjRBFQqJmQN"
      },
      "source": [
        "class Sample:\n",
        "    def __init__(self, question, context, q_ids=None, start_char_idx=None, answer_text=None, all_answers=None):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.q_ids = q_ids\n",
        "        self.skip = False\n",
        "        self.start_token_idx = -1\n",
        "        self.end_token_idx = -1\n",
        "\n",
        "    def preprocess(self):\n",
        "        # clean context and question\n",
        "        context = \" \".join(str(self.context).split())\n",
        "        question = \" \".join(str(self.question).split())\n",
        "        # tokenize context and question\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "        # if this is validation or training sample, preprocess answer\n",
        "        if self.answer_text is not None:\n",
        "            answer = \" \".join(str(self.answer_text).split())\n",
        "            # check if end character index is in the context\n",
        "            end_char_idx = self.start_char_idx + len(answer)\n",
        "            if end_char_idx >= len(context):\n",
        "                self.skip = True\n",
        "                return\n",
        "            # mark all the character indexes in context that are also in answer     \n",
        "            is_char_in_ans = [0] * len(context)\n",
        "            for idx in range(self.start_char_idx, end_char_idx):\n",
        "                is_char_in_ans[idx] = 1\n",
        "            ans_token_idx = []\n",
        "            # find all the tokens that are in the answers\n",
        "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "                if sum(is_char_in_ans[start:end]) > 0:\n",
        "                    ans_token_idx.append(idx)\n",
        "            if len(ans_token_idx) == 0:\n",
        "                self.skip = True\n",
        "                return\n",
        "            # get start and end token indexes\n",
        "            self.start_token_idx = ans_token_idx[0]\n",
        "            self.end_token_idx = ans_token_idx[-1]\n",
        "        # create inputs as usual\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        # add padding if necessary\n",
        "        if padding_length > 0:\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "        self.input_word_ids = input_ids\n",
        "        self.input_type_ids = token_type_ids\n",
        "        self.input_mask = attention_mask\n",
        "        self.context_token_to_char = tokenized_context.offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMfPlDE9gfVK"
      },
      "source": [
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                q_id = qa['id']\n",
        "                if \"answers\" in qa:\n",
        "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                    squad_eg = Sample(question, context, q_id,start_char_idx, answer_text, all_answers)\n",
        "                else:\n",
        "                    squad_eg = Sample(question, context, q_id)\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_word_ids\": [],\n",
        "        \"input_type_ids\": [],\n",
        "        \"input_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if not item.skip:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "    x = [dataset_dict[\"input_word_ids\"],\n",
        "         dataset_dict[\"input_mask\"],\n",
        "         dataset_dict[\"input_type_ids\"]]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IivIgDSB2eov"
      },
      "source": [
        "def train_eval(train_set, n=320):\n",
        "  results = []\n",
        "  indices = np.random.choice(len(train_set['data']), n)\n",
        " \n",
        "  eval_data = {}\n",
        "  train_data = {}\n",
        "  for i, item in enumerate(train_set['data']):\n",
        "    results.append(item)\n",
        "    \n",
        "  train_data['data'] = results[:n]\n",
        "  eval_data['data'] = results[n:]\n",
        "  return train_data, eval_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzxomSBoFz21"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6HhE13X2-HT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a636f975-b5ab-47ea-8507-3777c85318e5"
      },
      "source": [
        "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
        "with open(train_path) as f: train_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "30294016/30288272 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzU4Hnzp8JQC"
      },
      "source": [
        "train0, test = train_eval(train_data, 310)\n",
        "train, val = train_eval(train0, 220)\n",
        "# Train set\n",
        "#with open('train.json', 'w') as jobj:\n",
        "#  json.dump(train, jobj)\n",
        "\n",
        "# Validation data set\n",
        "with open('testset.json', 'w') as jobj:\n",
        "  json.dump(val, jobj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFf3hVDhgrfW",
        "outputId": "361652d8-d07f-4780-f116-3347e490189e"
      },
      "source": [
        "max_seq_length = 384\n",
        "\n",
        "# \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2\"\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
        "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
        "encoder_input = {'input_word_ids': input_word_ids, 'input_mask': input_mask, 'input_type_ids': input_type_ids}\n",
        "bert_layer = hub.KerasLayer( 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3', trainable=True)\n",
        "\n",
        "outputs = bert_layer(encoder_input)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenizers.BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
        "train_squad_examples = create_squad_examples(train)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "eval_squad_examples = create_squad_examples(val)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
        "test_squad_examples = create_squad_examples(test)\n",
        "x_test, y_test = create_inputs_targets(test_squad_examples)\n",
        "print(f\"{len(test_squad_examples)} test points created.\")\n",
        "\n",
        "\n",
        "sequence_output = outputs['sequence_output']\n",
        "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
        "start_logits = layers.Flatten()(start_logits)\n",
        "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
        "end_logits = layers.Flatten()(end_logits)\n",
        "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45819 training points created.\n",
            "16375 evaluation points created.\n",
            "25405 test points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9pSdjwTpo57"
      },
      "source": [
        "def normalize_text(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "  # remove redundant whitespaces\n",
        "  text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
        "  # remove articles\n",
        "  regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "  text = re.sub(regex, \" \", text)\n",
        "  text = \" \".join(text.split())\n",
        "  return text\n",
        "\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s: return []\n",
        "    return normalize_text(s).split()\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeC27RPspub_"
      },
      "source": [
        "def accuracy(pred_start, pred_end, data):\n",
        "  count = 0\n",
        "  f1 = 0\n",
        "  N = len(pred_end)\n",
        "  eval_examples_no_skip = [_ for _ in data if _.skip == False]\n",
        "  for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    # take the required Sample object with the ground-truth answers in it\n",
        "    squad_eg = eval_examples_no_skip[idx]\n",
        "    # use offsets to get back the span of text corresponding to\n",
        "    # our predicted first and last tokens\n",
        "    offsets = squad_eg.context_token_to_char\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    if start >= len(offsets):\n",
        "      continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "      pred_char_end = offsets[end][1]\n",
        "      pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "    else:\n",
        "      pred_ans = squad_eg.context[pred_char_start:]\n",
        "    normalized_pred_ans = normalize_text(pred_ans)\n",
        "    # clean the real answers\n",
        "    normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "    # check if the predicted answer is in an array of the ground-truth answers\n",
        "    if normalized_pred_ans in normalized_true_ans:\n",
        "      count += 1\n",
        "    f1 += max(compute_f1(normalized_pred_ans, x) for x in normalized_true_ans)\n",
        "    \n",
        "  acc = count / N\n",
        "  f1_score = f1/N\n",
        "  return acc, f1_score\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztqAvMYqJAa"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss_tracker = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "def train_step(data, model):\n",
        "  X, y = data\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(X, training=True)\n",
        "    # compute loss\n",
        "    loss = loss_tracker(y, y_pred)\n",
        "    \n",
        "  # compute gradients\n",
        "  # trainable_vars = self.trainable_variables\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  # Update weights\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  # loss_tracker.update_state(loss)\n",
        "  # pred_start, pred_end = y_pred[0], y_pred[1]\n",
        "  \n",
        "  # acc = accuracy(pred_start, pred_end)\n",
        "  return {'loss': loss}\n",
        "\n",
        "def test_step(data, model):\n",
        "  X, y = data\n",
        "  y_pred = model.predict(X)\n",
        "  # loss\n",
        "  val_loss = loss_tracker(y, y_pred)\n",
        "  pred_start, pred_end = y_pred\n",
        "  val_acc, f1 = accuracy(pred_start, pred_end, eval_squad_examples)\n",
        "  return {'val_loss': val_loss, 'val_acc': val_acc, 'f1_score': f1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjRuDhKxqNOO"
      },
      "source": [
        "def train(model, X_train, X_val, epochs=6, train_steps=None, val_steps=None):\n",
        "  x_train, y_train = X_train\n",
        "  x_val, y_val = X_val\n",
        "  # x_eval, y_eval = X_val\n",
        "  loss = []\n",
        "  exact = []\n",
        "  f1_score = []\n",
        "  # accuracy = []\n",
        "  val_loss = []\n",
        "  val_exact = []\n",
        "  val_f1_score = []\n",
        "\n",
        "  # train and val sets\n",
        "  data_x = list(map(lambda x: x[:train_steps], x_train))\n",
        "  # data_y = list(map(lambda x: x[:train_steps], x_train))\n",
        "  x_val = list(map(lambda x: x[:val_steps], x_val))\n",
        "  y_val = list(map(lambda x: x[:val_steps], y_val))\n",
        "  print('start training...')\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    temp_loss = 0\n",
        "    # temp_acc = 0\n",
        "    start = time.time()\n",
        "    for i in range(1, len(x_train[0])-1):\n",
        "      train_ds = ([x_train[0][i-1:i], x_train[1][i-1:i], x_train[2][i-1:i]], [y_train[0][i-1:i], y_train[1][i-1:i]])\n",
        "      results = train_step(train_ds, model)\n",
        "      temp_loss += results['loss'].numpy()\n",
        "      # temp_acc += results['accuracy']\n",
        "      \n",
        "      # stop\n",
        "      if train_steps is not None:\n",
        "        if i == train_steps:\n",
        "          break\n",
        "    # saving tempory statistics\n",
        "    if train_steps is not None:\n",
        "      loss.append(temp_loss/train_steps)\n",
        "    else:\n",
        "      return\n",
        "    #\n",
        "    \n",
        "    y_pred = model.predict(data_x)\n",
        "    pred_start, pred_end = y_pred[0], y_pred[1]\n",
        "    acc = accuracy(pred_start, pred_end, train_squad_examples)\n",
        "    exact.append(acc[0])\n",
        "    f1_score.append(acc[1])\n",
        "    #\n",
        "    if val_steps is not None:\n",
        "      print('prediction...')\n",
        "\n",
        "      val_results = test_step((x_val, y_val), model)\n",
        "      val_loss.append(val_results['val_loss'])\n",
        "      val_exact.append(val_results['val_acc'])\n",
        "      val_f1_score.append(val_results['f1_score'])\n",
        "    else:\n",
        "      return\n",
        "    end = time.time()\n",
        "\n",
        "    # show performance after every epoch\n",
        "    print(f\"Time used: {end-start}\\tloss: {temp_loss/train_steps:.4f}\\texact: {exact[-1]:.4f}\\tf1_score: {f1_score[-1]:.4f}\\tval_loss: {val_loss[-1]:.4f}\\tval_EM: {val_exact[-1]}\\tf1_score: {f1_score[-1]:.4f}\\n\")\n",
        "    \n",
        "    \n",
        "  return {'loss': loss, 'exact_match': exact, 'f1_score': f1_score, 'val_loss': [x.numpy() for x in val_loss], 'val_EM': val_exact, 'val_f1_score': val_f1_score}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFNg_XQM83hg"
      },
      "source": [
        "large_bert_model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_w3l8SCqQbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb419a45-6c46-4715-ed57-7eddb20063a6"
      },
      "source": [
        "history = train(large_bert_model, (x_train, y_train), (x_eval, y_eval),epochs=10, train_steps=9000, val_steps=3500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training...\n",
            "Epoch: 1\n",
            "prediction...\n",
            "Time used: 1202.8177497386932\tloss: 1.4868\texact: 0.7161\tf1_score: 0.8258\tval_loss: 1.6794\tval_EM: 0.5177142857142857\tf1_score: 0.8258\n",
            "\n",
            "Epoch: 2\n",
            "prediction...\n",
            "Time used: 1190.0906629562378\tloss: 0.8072\texact: 0.8126\tf1_score: 0.8923\tval_loss: 2.0317\tval_EM: 0.5145714285714286\tf1_score: 0.8923\n",
            "\n",
            "Epoch: 3\n",
            "prediction...\n",
            "Time used: 1190.8402161598206\tloss: 0.5162\texact: 0.8636\tf1_score: 0.9244\tval_loss: 2.7078\tval_EM: 0.5291428571428571\tf1_score: 0.9244\n",
            "\n",
            "Epoch: 4\n",
            "prediction...\n",
            "Time used: 1193.1886096000671\tloss: 0.3842\texact: 0.8954\tf1_score: 0.9409\tval_loss: 2.6723\tval_EM: 0.5251428571428571\tf1_score: 0.9409\n",
            "\n",
            "Epoch: 5\n",
            "prediction...\n",
            "Time used: 1192.5160248279572\tloss: 0.2870\texact: 0.9209\tf1_score: 0.9558\tval_loss: 2.7378\tval_EM: 0.5257142857142857\tf1_score: 0.9558\n",
            "\n",
            "Epoch: 6\n",
            "prediction...\n",
            "Time used: 1192.6166143417358\tloss: 0.2390\texact: 0.9228\tf1_score: 0.9567\tval_loss: 2.8613\tval_EM: 0.5231428571428571\tf1_score: 0.9567\n",
            "\n",
            "Epoch: 7\n",
            "prediction...\n",
            "Time used: 1190.9814219474792\tloss: 0.2087\texact: 0.9324\tf1_score: 0.9613\tval_loss: 3.1518\tval_EM: 0.5165714285714286\tf1_score: 0.9613\n",
            "\n",
            "Epoch: 8\n",
            "prediction...\n",
            "Time used: 1195.0908682346344\tloss: 0.1802\texact: 0.9383\tf1_score: 0.9648\tval_loss: 2.8139\tval_EM: 0.5185714285714286\tf1_score: 0.9648\n",
            "\n",
            "Epoch: 9\n",
            "prediction...\n",
            "Time used: 1192.629781961441\tloss: 0.1545\texact: 0.9454\tf1_score: 0.9697\tval_loss: 3.8381\tval_EM: 0.5142857142857142\tf1_score: 0.9697\n",
            "\n",
            "Epoch: 10\n",
            "prediction...\n",
            "Time used: 1192.44127035141\tloss: 0.1425\texact: 0.9414\tf1_score: 0.9673\tval_loss: 3.5147\tval_EM: 0.5131428571428571\tf1_score: 0.9673\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oBFpI26qRcV"
      },
      "source": [
        "def Save_performance(history, name='Electra'):\n",
        "  history = {k: [float(x) for x in v] for k, v in history.items()}\n",
        "  with open(name+'.json', 'w') as obj:\n",
        "    json.dump(history, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgGuTybrqfTP"
      },
      "source": [
        "Save_performance(history, name='large_bert_history')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THwmc8JvAtnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "345f90c0-8dcf-47bf-d20e-184029f2faca"
      },
      "source": [
        "large_bert_model.save('large_bert_saved_model', include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 910). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 910). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: large_bert_saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: large_bert_saved_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8IN7dK_uKsH",
        "outputId": "2a4b7745-e2b3-467a-884d-b5e6d15905e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aDJ_bRTBLS7"
      },
      "source": [
        "# retreived_model = tf.keras.models.load_model('electra_saved_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-iFB7hQ5Erq"
      },
      "source": [
        "def generate_prediction(model, data):\n",
        "    # Stored results\n",
        "    \n",
        "    ######\n",
        "    print('Create samples...')\n",
        "    sample_examples = create_squad_examples(data)\n",
        "    print('Samples creation completed...')\n",
        "    print('Create input data...')\n",
        "    x_eval, _ = create_inputs_targets(sample_examples)\n",
        "    # get the offsets of the first and last tokens of predicted answers\n",
        "    st = time.time()\n",
        "    pred_start, pred_end = model.predict(x_eval)\n",
        "    ed = time.time()\n",
        "    print(f\"Time for prediction: {(ed - st)}s\")\n",
        "    count = 0\n",
        "    pred_ans = None\n",
        "    eval_examples_no_skip = [_ for _ in sample_examples if _.skip == False]\n",
        "    # for every pair of offsets\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "        # take the required Sample object with the ground-truth answers in it\n",
        "        squad_eg = eval_examples_no_skip[idx]\n",
        "        # use offsets to get back the span of text corresponding to\n",
        "        # our predicted first and last tokens\n",
        "        offsets = squad_eg.context_token_to_char\n",
        "        # Get the best i.e max\n",
        "        start = np.argmax(start)\n",
        "        end = np.argmax(end)\n",
        "        if start >= len(offsets):\n",
        "            continue\n",
        "        pred_char_start = offsets[start][0]\n",
        "        if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "        else:\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "        yield squad_eg.q_ids, pred_ans\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "def save(generator):\n",
        "    import json\n",
        "    with open('large_bert_prediction.json', 'w') as p:\n",
        "        json.dump(dict([i for i in generator]), p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5CwBdtXBU3j"
      },
      "source": [
        "# save(generate_prediction(electra_model, raw_eval_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEBkg0cN1kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5fdaa4-767f-4015-cdd9-68ccfdce65e6"
      },
      "source": [
        "save(generate_prediction(large_bert_model, test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create samples...\n",
            "Samples creation completed...\n",
            "Create input data...\n",
            "Time for prediction: 373.7849395275116s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKrLTZjUXTkr"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "folder = os.path.join('drive', 'MyDrive', 'saved_nlp_models')\n",
        "if not os.path.exists(folder):\n",
        "  os.makedirs(folder)\n",
        "shutil.move('large_bert_saved_model', folder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRhAZ1tprd_X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "095c8b40-02a1-44a9-ae28-608954df84a4"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "folder = os.path.join('drive', 'MyDrive', 'saved_nlp_models')\n",
        "if not os.path.exists(folder):\n",
        "  os.makedirs(folder)\n",
        "shutil.move('large_bert_saved_model', folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/saved_nlp_models/large_bert_saved_model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ctY_KPrSbg"
      },
      "source": [
        "def generate_prediction_(model, data):\n",
        "    # Stored results\n",
        "    res = {}\n",
        "    count = 0\n",
        "    ######\n",
        "    print('Create samples...')\n",
        "    sample_examples = create_squad_examples(data)\n",
        "    print('Samples creation completed...')\n",
        "    print('Create input data...')\n",
        "    x_eval, _ = create_inputs_targets(sample_examples)\n",
        "    # get the offsets of the first and last tokens of predicted answers\n",
        "    st = time.time()\n",
        "    pred_start, pred_end = model.predict(x_eval)\n",
        "    ed = time.time()\n",
        "    print(f\"Time for prediction: {(ed - st)}s\")\n",
        "    count = 0\n",
        "    pred_ans = None\n",
        "\n",
        "    eval_examples_no_skip = [_ for _ in sample_examples if _.skip == False]\n",
        "    # for every pair of offsets\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "        # take the required Sample object with the ground-truth answers in it\n",
        "        squad_eg = eval_examples_no_skip[idx]\n",
        "        # use offsets to get back the span of text corresponding to\n",
        "        # our predicted first and last tokens\n",
        "        offsets = squad_eg.context_token_to_char\n",
        "        q_id = squad_eg.q_ids\n",
        "        # Get the best i.e max\n",
        "        start = np.argmax(start)\n",
        "        end = np.argmax(end)\n",
        "        if start >= len(offsets):\n",
        "            continue\n",
        "        pred_char_start = offsets[start][0]\n",
        "        if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "        else:\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\n",
        "        normalized_pred_ans = normalize_text(pred_ans)\n",
        "        normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "        if normalized_pred_ans not in normalized_true_ans:\n",
        "          count += 1\n",
        "          res[q_id] = normalized_pred_ans\n",
        "    return res\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8G3ED3dsnz7"
      },
      "source": [
        "import os\n",
        "model_name = ['expert_saved_model', 'baseline_saved_model', 'electra_saved_model', 'large_bert_saved_model']\n",
        "folder = os.path.join('drive', 'MyDrive', 'saved_nlp_models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dZzdQaNsdbe"
      },
      "source": [
        "model = tf.keras.models.load_model(os.path.join(folder, model_name[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKfnktVUtP4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3504ec9f-360d-4264-ecbf-a87d6c6a4367"
      },
      "source": [
        "results = generate_prediction_(large_bert_model, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create samples...\n",
            "Samples creation completed...\n",
            "Create input data...\n",
            "Time for prediction: 373.9064474105835s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJtc87c8wC1o"
      },
      "source": [
        "with open('large_bert_wrongly_classified.json', 'w') as obj:\n",
        "  json.dump(results, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujQm5vhsLvN8"
      },
      "source": [
        "def get_raw_scores(dataset):\n",
        "    ground_truth = {}\n",
        "    for article in dataset['data']:\n",
        "        for p in article['paragraphs']:\n",
        "            for qa in p['qas']:\n",
        "                qid = qa['id']\n",
        "                gold_answers = [a['text'] for a in qa['answers']\n",
        "                                if normalize_text(a['text'])]\n",
        "                ground_truth[qid] = [qa['question'], gold_answers]\n",
        "    return ground_truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFgVHgXrN7vk"
      },
      "source": [
        "ground_truth = get_raw_scores(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywnu6U8cOP0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5cd112-ff7d-4980-e650-6bfc4e5632a4"
      },
      "source": [
        "len(ground_truth.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25405"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUmjwltoOWdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b71129-8b4a-4d14-ae92-7c19777bad60"
      },
      "source": [
        "print(f\"Number of misclassified: {len(results.keys())}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of misclassified: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9qNI1TaOjPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82b3498-3487-4e42-d964-450ba1cfaeb7"
      },
      "source": [
        "for i, (k,v) in enumerate(ground_truth.items()):\n",
        "  if k in results:\n",
        "    print(f\"id: {k}\\nQuestion: {v[0]}\\nGround truth: {v[1][0]}\\nPrediction: {results[k]}\\n\")\n",
        "  if i==50:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id: 5728027d3acd2414000df20d\n",
            "Question: What year was the PlayStation 3 released?\n",
            "Ground truth: 2006\n",
            "Prediction: 2009\n",
            "\n",
            "id: 5728027d3acd2414000df20f\n",
            "Question: What was the thinner version of the PS3 called?\n",
            "Ground truth: Slim\n",
            "Prediction: \n",
            "\n",
            "id: 5728027d3acd2414000df210\n",
            "Question: What year did the Super Slim model hit stores?\n",
            "Ground truth: 2012\n",
            "Prediction: late 2012\n",
            "\n",
            "id: 572805603acd2414000df279\n",
            "Question: What popular game was demoed in a video at the game shows?\n",
            "Ground truth: Final Fantasy VII\n",
            "Prediction: fantasy vii tech\n",
            "\n",
            "id: 572805603acd2414000df27a\n",
            "Question: What event did Sony take the PlayStation 3 to four months after E3?\n",
            "Ground truth: Tokyo Game Show\n",
            "Prediction: e3 2005\n",
            "\n",
            "id: 572805603acd2414000df27b\n",
            "Question: What was one game Sony debuted on a modified PC so gamers could get a look?\n",
            "Ground truth: Metal Gear Solid 4: Guns of the Patriots\n",
            "Prediction: playstation 3\n",
            "\n",
            "id: 5728066eff5b5019007d9b2c\n",
            "Question: Why might Sony have reduced the number of ports on the PlayStation 3 before production?\n",
            "Ground truth: to cut costs\n",
            "Prediction: cut costs\n",
            "\n",
            "id: 572807bb3acd2414000df2b1\n",
            "Question: Which region experience a setback that pushed back the release of the PlayStation 3?\n",
            "Ground truth: PAL\n",
            "Prediction: pal region\n",
            "\n",
            "id: 572807bb3acd2414000df2b2\n",
            "Question: What part of the system was Sony having trouble getting supplies for?\n",
            "Ground truth: Blu-ray drive\n",
            "Prediction: \n",
            "\n",
            "id: 572807bb3acd2414000df2b3\n",
            "Question: Which Japanese PS3 model got a 20%-plus price cut before hitting the market?\n",
            "Ground truth: 20 GB model\n",
            "Prediction: 20 gb\n",
            "\n",
            "id: 57280ac93acd2414000df2fd\n",
            "Question: Instead of November, for what month of the following year was the release rescheduled?\n",
            "Ground truth: March\n",
            "Prediction: september\n",
            "\n",
            "id: 57280cc73acd2414000df319\n",
            "Question: What was the model number of the slim version of the PlayStation 3?\n",
            "Ground truth: CECH-2000\n",
            "Prediction: ps3\n",
            "\n",
            "id: 57280cc73acd2414000df31b\n",
            "Question: What would customers notice about the sound of the new, improved cooling system?\n",
            "Ground truth: quieter\n",
            "Prediction: slimmer form factor decreased power consumption\n",
            "\n",
            "id: 57280cc73acd2414000df31c\n",
            "Question: What did the boot screen on the game consoles read before Sony changed it to \"PS3 PlayStation 3\"?\n",
            "Ground truth: \"Sony Computer Entertainment\"\n",
            "Prediction: chime\n",
            "\n",
            "id: 572810633acd2414000df386\n",
            "Question: What's the name of the sequel game to MotorStorm?\n",
            "Ground truth: MotorStorm: Pacific Rift\n",
            "Prediction: resistance 2 and motorstorm pacific rift\n",
            "\n",
            "id: 572814c34b864d190016441e\n",
            "Question: Which Ratchet & Clank title debuted at E3 2007?\n",
            "Ground truth: Ratchet & Clank Future: Tools of Destruction\n",
            "Prediction: future tools of destruction warhawk\n",
            "\n",
            "id: 572814c34b864d190016441f\n",
            "Question: What year was Warhawk released for the PlayStation 3?\n",
            "Ground truth: 2007\n",
            "Prediction: \n",
            "\n",
            "id: 572814c34b864d1900164421\n",
            "Question: What Gran Turismo game was shown in 2007 but not released until after 2007?\n",
            "Ground truth: Gran Turismo 5 Prologue\n",
            "Prediction: gran turismo 5\n",
            "\n",
            "id: 572814c34b864d1900164422\n",
            "Question: Which much anticipated third-party game with the name of a month of the year in it did Sony show at E3 2007?\n",
            "Ground truth: Devil May Cry 4\n",
            "Prediction: \n",
            "\n",
            "id: 5728162d4b864d1900164440\n",
            "Question: What's Sony's budget line of PS3 games called in Japan?\n",
            "Ground truth: The Best\n",
            "Prediction: best range\n",
            "\n",
            "id: 5728162d4b864d1900164444\n",
            "Question: What words would you see in the United States or Canada on a PS3 game that would signify its lower price?\n",
            "Ground truth: Greatest Hits\n",
            "Prediction: platinum range in europe and australia and best range in japan\n",
            "\n",
            "id: 5728162d4b864d1900164442\n",
            "Question: Which Call of Duty title does Sony include in their low-end price range?\n",
            "Ground truth: Call Of Duty 3\n",
            "Prediction: \n",
            "\n",
            "id: 572817fb2ca10214002d9dba\n",
            "Question: What company said it would bring 3D technology to the PS3?\n",
            "Ground truth: Blitz Games\n",
            "Prediction: cto of blitz games\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3puQVD7bZn-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186af6cc-19d2-46e6-8bc4-ba2e419ec4fa"
      },
      "source": [
        "print(f\"Proportion of wrongly classifier: {len(results.keys())/len(ground_truth.keys())}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportion of wrongly classifier: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwrxzSJgKl1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}