{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Expert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7sypFcvvZHX"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j68pPERtkE5v",
        "outputId": "3ececc99-01d6-4096-de44-02edc170df61"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 12.9MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5tRwq9f5OIF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import json\n",
        "import collections\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import tokenizers\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import string \n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACjRBFQqJmQN"
      },
      "source": [
        "class Sample:\n",
        "    def __init__(self, question, context, q_ids=None, start_char_idx=None, answer_text=None, all_answers=None):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.q_ids = q_ids\n",
        "        self.skip = False\n",
        "        self.start_token_idx = -1\n",
        "        self.end_token_idx = -1\n",
        "\n",
        "    def preprocess(self):\n",
        "        # clean context and question\n",
        "        context = \" \".join(str(self.context).split())\n",
        "        question = \" \".join(str(self.question).split())\n",
        "        # tokenize context and question\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "        # if this is validation or training sample, preprocess answer\n",
        "        if self.answer_text is not None:\n",
        "            answer = \" \".join(str(self.answer_text).split())\n",
        "            # check if end character index is in the context\n",
        "            end_char_idx = self.start_char_idx + len(answer)\n",
        "            if end_char_idx >= len(context):\n",
        "                self.skip = True\n",
        "                return\n",
        "            # mark all the character indexes in context that are also in answer     \n",
        "            is_char_in_ans = [0] * len(context)\n",
        "            for idx in range(self.start_char_idx, end_char_idx):\n",
        "                is_char_in_ans[idx] = 1\n",
        "            ans_token_idx = []\n",
        "            # find all the tokens that are in the answers\n",
        "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "                if sum(is_char_in_ans[start:end]) > 0:\n",
        "                    ans_token_idx.append(idx)\n",
        "            if len(ans_token_idx) == 0:\n",
        "                self.skip = True\n",
        "                return\n",
        "            # get start and end token indexes\n",
        "            self.start_token_idx = ans_token_idx[0]\n",
        "            self.end_token_idx = ans_token_idx[-1]\n",
        "        # create inputs as usual\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        # add padding if necessary\n",
        "        if padding_length > 0:\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "        self.input_word_ids = input_ids\n",
        "        self.input_type_ids = token_type_ids\n",
        "        self.input_mask = attention_mask\n",
        "        self.context_token_to_char = tokenized_context.offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMfPlDE9gfVK"
      },
      "source": [
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                q_id = qa['id']\n",
        "                if \"answers\" in qa:\n",
        "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                    squad_eg = Sample(question, context, q_id,start_char_idx, answer_text, all_answers)\n",
        "                else:\n",
        "                    squad_eg = Sample(question, context, q_id)\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_word_ids\": [],\n",
        "        \"input_type_ids\": [],\n",
        "        \"input_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if not item.skip:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "    x = [dataset_dict[\"input_word_ids\"],\n",
        "         dataset_dict[\"input_mask\"],\n",
        "         dataset_dict[\"input_type_ids\"]]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IivIgDSB2eov"
      },
      "source": [
        "def train_eval(train_set, n=320):\n",
        "  results = []\n",
        "  indices = np.random.choice(len(train_set['data']), n)\n",
        " \n",
        "  eval_data = {}\n",
        "  train_data = {}\n",
        "  for i, item in enumerate(train_set['data']):\n",
        "    results.append(item)\n",
        "    \n",
        "  train_data['data'] = results[:n]\n",
        "  eval_data['data'] = results[n:]\n",
        "  return train_data, eval_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzxomSBoFz21"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6HhE13X2-HT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96039240-fa81-4f19-d153-d5feca0af413"
      },
      "source": [
        "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
        "with open(train_path) as f: train_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "30294016/30288272 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzU4Hnzp8JQC"
      },
      "source": [
        "train0, test = train_eval(train_data, 310)\n",
        "train, val = train_eval(train0, 220)\n",
        "# Train set\n",
        "#with open('train.json', 'w') as jobj:\n",
        "#  json.dump(train, jobj)\n",
        "\n",
        "# Validation data set\n",
        "with open('testset.json', 'w') as jobj:\n",
        "  json.dump(val, jobj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFf3hVDhgrfW",
        "outputId": "495d554e-6dea-4d43-9af9-7eddb2a40716"
      },
      "source": [
        "max_seq_length = 384\n",
        "\n",
        "# \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2\"\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
        "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
        "encoder_input = {'input_word_ids': input_word_ids, 'input_mask': input_mask, 'input_type_ids': input_type_ids}\n",
        "bert_layer = hub.KerasLayer('https://tfhub.dev/google/experts/bert/wiki_books/2', trainable=True)\n",
        "\n",
        "outputs = bert_layer(encoder_input)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenizers.BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
        "train_squad_examples = create_squad_examples(train)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "eval_squad_examples = create_squad_examples(val)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
        "test_squad_examples = create_squad_examples(test)\n",
        "x_test, y_test = create_inputs_targets(test_squad_examples)\n",
        "print(f\"{len(test_squad_examples)} test points created.\")\n",
        "\n",
        "\n",
        "sequence_output = outputs['sequence_output']\n",
        "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
        "start_logits = layers.Flatten()(start_logits)\n",
        "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
        "end_logits = layers.Flatten()(end_logits)\n",
        "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45819 training points created.\n",
            "16375 evaluation points created.\n",
            "25405 test points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9pSdjwTpo57"
      },
      "source": [
        "def normalize_text(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "  # remove redundant whitespaces\n",
        "  text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
        "  # remove articles\n",
        "  regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "  text = re.sub(regex, \" \", text)\n",
        "  text = \" \".join(text.split())\n",
        "  return text\n",
        "\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s: return []\n",
        "    return normalize_text(s).split()\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeC27RPspub_"
      },
      "source": [
        "def accuracy(pred_start, pred_end, data):\n",
        "  count = 0\n",
        "  f1 = 0\n",
        "  N = len(pred_end)\n",
        "  eval_examples_no_skip = [_ for _ in data if _.skip == False]\n",
        "  for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    # take the required Sample object with the ground-truth answers in it\n",
        "    squad_eg = eval_examples_no_skip[idx]\n",
        "    # use offsets to get back the span of text corresponding to\n",
        "    # our predicted first and last tokens\n",
        "    offsets = squad_eg.context_token_to_char\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    if start >= len(offsets):\n",
        "      continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "      pred_char_end = offsets[end][1]\n",
        "      pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "    else:\n",
        "      pred_ans = squad_eg.context[pred_char_start:]\n",
        "    normalized_pred_ans = normalize_text(pred_ans)\n",
        "    # clean the real answers\n",
        "    normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "    # check if the predicted answer is in an array of the ground-truth answers\n",
        "    if normalized_pred_ans in normalized_true_ans:\n",
        "      count += 1\n",
        "    f1 += max(compute_f1(normalized_pred_ans, x) for x in normalized_true_ans)\n",
        "    \n",
        "  acc = count / N\n",
        "  f1_score = f1/N\n",
        "  return acc, f1_score\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztqAvMYqJAa"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss_tracker = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "def train_step(data, model):\n",
        "  X, y = data\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(X, training=True)\n",
        "    # compute loss\n",
        "    loss = loss_tracker(y, y_pred)\n",
        "    \n",
        "  # compute gradients\n",
        "  # trainable_vars = self.trainable_variables\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  # Update weights\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  # loss_tracker.update_state(loss)\n",
        "  # pred_start, pred_end = y_pred[0], y_pred[1]\n",
        "  \n",
        "  # acc = accuracy(pred_start, pred_end)\n",
        "  return {'loss': loss}\n",
        "\n",
        "def test_step(data, model):\n",
        "  X, y = data\n",
        "  y_pred = model.predict(X)\n",
        "  # loss\n",
        "  val_loss = loss_tracker(y, y_pred)\n",
        "  pred_start, pred_end = y_pred\n",
        "  val_acc, f1 = accuracy(pred_start, pred_end, eval_squad_examples)\n",
        "  return {'val_loss': val_loss, 'val_acc': val_acc, 'f1_score': f1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjRuDhKxqNOO"
      },
      "source": [
        "def train(model, X_train, X_val, epochs=10, train_steps=None, val_steps=None):\n",
        "  x_train, y_train = X_train\n",
        "  x_val, y_val = X_val\n",
        "  # x_eval, y_eval = X_val\n",
        "  loss = []\n",
        "  exact = []\n",
        "  f1_score = []\n",
        "  # accuracy = []\n",
        "  val_loss = []\n",
        "  val_exact = []\n",
        "  val_f1_score = []\n",
        "\n",
        "  # train and val sets\n",
        "  data_x = list(map(lambda x: x[:train_steps], x_train))\n",
        "  # data_y = list(map(lambda x: x[:train_steps], x_train))\n",
        "  x_val = list(map(lambda x: x[:val_steps], x_val))\n",
        "  y_val = list(map(lambda x: x[:val_steps], y_val))\n",
        "  print('start training...')\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    temp_loss = 0\n",
        "    # temp_acc = 0\n",
        "    start = time.time()\n",
        "    for i in range(1, len(x_train[0])-1):\n",
        "      train_ds = ([x_train[0][i-1:i], x_train[1][i-1:i], x_train[2][i-1:i]], [y_train[0][i-1:i], y_train[1][i-1:i]])\n",
        "      results = train_step(train_ds, model)\n",
        "      temp_loss += results['loss'].numpy()\n",
        "      # temp_acc += results['accuracy']\n",
        "      \n",
        "      # stop\n",
        "      if train_steps is not None:\n",
        "        if i == train_steps:\n",
        "          break\n",
        "    # saving tempory statistics\n",
        "    if train_steps is not None:\n",
        "      loss.append(temp_loss/train_steps)\n",
        "    else:\n",
        "      return\n",
        "    #\n",
        "    \n",
        "    y_pred = model.predict(data_x)\n",
        "    pred_start, pred_end = y_pred[0], y_pred[1]\n",
        "    acc = accuracy(pred_start, pred_end, train_squad_examples)\n",
        "    exact.append(acc[0])\n",
        "    f1_score.append(acc[1])\n",
        "    #\n",
        "    if val_steps is not None:\n",
        "      print('prediction...')\n",
        "\n",
        "      val_results = test_step((x_val, y_val), model)\n",
        "      val_loss.append(val_results['val_loss'])\n",
        "      val_exact.append(val_results['val_acc'])\n",
        "      val_f1_score.append(val_results['f1_score'])\n",
        "    else:\n",
        "      return\n",
        "    end = time.time()\n",
        "\n",
        "    # show performance after every epoch\n",
        "    print(f\"Time used: {end-start}\\tloss: {temp_loss/train_steps:.4f}\\texact: {exact[-1]:.4f}\\tf1_score: {f1_score[-1]:.4f}\\tval_loss: {val_loss[-1]:.4f}\\tval_EM: {val_exact[-1]}\\tf1_score: {f1_score[-1]:.4f}\\n\")\n",
        "    \n",
        "    \n",
        "  return {'loss': loss, 'exact_match': exact, 'f1_score': f1_score, 'val_loss': [x.numpy() for x in val_loss], 'val_EM': val_exact, 'val_f1_score': val_f1_score}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFNg_XQM83hg"
      },
      "source": [
        "expert_model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_w3l8SCqQbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9242a22-cd0d-4b5b-a8be-bb9fa6bbfea4"
      },
      "source": [
        "history = train(expert_model, (x_train, y_train), (x_eval, y_eval), train_steps=9000, val_steps=3500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training...\n",
            "Epoch: 1\n",
            "prediction...\n",
            "Time used: 1176.3748273849487\tloss: 1.3154\texact: 0.7492\tf1_score: 0.8557\tval_loss: 1.4773\tval_EM: 0.5728571428571428\tf1_score: 0.8557\n",
            "\n",
            "Epoch: 2\n",
            "prediction...\n",
            "Time used: 1165.0669696331024\tloss: 0.6957\texact: 0.8360\tf1_score: 0.9110\tval_loss: 1.6327\tval_EM: 0.5922857142857143\tf1_score: 0.9110\n",
            "\n",
            "Epoch: 3\n",
            "prediction...\n",
            "Time used: 1162.6282324790955\tloss: 0.4458\texact: 0.8807\tf1_score: 0.9359\tval_loss: 2.0069\tval_EM: 0.5945714285714285\tf1_score: 0.9359\n",
            "\n",
            "Epoch: 4\n",
            "prediction...\n",
            "Time used: 1163.704538822174\tloss: 0.3059\texact: 0.9020\tf1_score: 0.9459\tval_loss: 2.1973\tval_EM: 0.5802857142857143\tf1_score: 0.9459\n",
            "\n",
            "Epoch: 5\n",
            "prediction...\n",
            "Time used: 1162.6429710388184\tloss: 0.2301\texact: 0.9297\tf1_score: 0.9604\tval_loss: 2.5074\tval_EM: 0.5965714285714285\tf1_score: 0.9604\n",
            "\n",
            "Epoch: 6\n",
            "prediction...\n",
            "Time used: 1155.6831722259521\tloss: 0.1780\texact: 0.9321\tf1_score: 0.9583\tval_loss: 2.8076\tval_EM: 0.5828571428571429\tf1_score: 0.9583\n",
            "\n",
            "Epoch: 7\n",
            "prediction...\n",
            "Time used: 1154.5531170368195\tloss: 0.1491\texact: 0.9333\tf1_score: 0.9613\tval_loss: 3.0466\tval_EM: 0.5902857142857143\tf1_score: 0.9613\n",
            "\n",
            "Epoch: 8\n",
            "prediction...\n",
            "Time used: 1154.8057751655579\tloss: 0.1403\texact: 0.9443\tf1_score: 0.9644\tval_loss: 2.7601\tval_EM: 0.5882857142857143\tf1_score: 0.9644\n",
            "\n",
            "Epoch: 9\n",
            "prediction...\n",
            "Time used: 1155.9228284358978\tloss: 0.1172\texact: 0.9467\tf1_score: 0.9647\tval_loss: 3.0073\tval_EM: 0.5808571428571428\tf1_score: 0.9647\n",
            "\n",
            "Epoch: 10\n",
            "prediction...\n",
            "Time used: 1150.830514907837\tloss: 0.1030\texact: 0.9481\tf1_score: 0.9666\tval_loss: 3.2606\tval_EM: 0.5845714285714285\tf1_score: 0.9666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oBFpI26qRcV"
      },
      "source": [
        "def Save_performance(history, name='Electra'):\n",
        "  history = {k: [float(x) for x in v] for k, v in history.items()}\n",
        "  with open(name+'.json', 'w') as obj:\n",
        "    json.dump(history, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgGuTybrqfTP"
      },
      "source": [
        "Save_performance(history, name='expert_history')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aDJ_bRTBLS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157413ae-6b9d-4e4b-ac06-28db40234e88"
      },
      "source": [
        "expert_model.save('expert_saved_model', include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 900). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 900). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: expert_saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: expert_saved_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-iFB7hQ5Erq"
      },
      "source": [
        "def generate_prediction(model, data):\n",
        "    # Stored results\n",
        "    \n",
        "    ######\n",
        "    print('Create samples...')\n",
        "    sample_examples = create_squad_examples(data)\n",
        "    print('Samples creation completed...')\n",
        "    print('Create input data...')\n",
        "    x_eval, _ = create_inputs_targets(sample_examples)\n",
        "    # get the offsets of the first and last tokens of predicted answers\n",
        "    st = time.time()\n",
        "    pred_start, pred_end = model.predict(x_eval)\n",
        "    ed = time.time()\n",
        "    print(f\"Time for prediction: {(ed - st)}s\")\n",
        "    count = 0\n",
        "    pred_ans = None\n",
        "    eval_examples_no_skip = [_ for _ in sample_examples if _.skip == False]\n",
        "    # for every pair of offsets\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "        # take the required Sample object with the ground-truth answers in it\n",
        "        squad_eg = eval_examples_no_skip[idx]\n",
        "        # use offsets to get back the span of text corresponding to\n",
        "        # our predicted first and last tokens\n",
        "        offsets = squad_eg.context_token_to_char\n",
        "        # Get the best i.e max\n",
        "        start = np.argmax(start)\n",
        "        end = np.argmax(end)\n",
        "        if start >= len(offsets):\n",
        "            continue\n",
        "        pred_char_start = offsets[start][0]\n",
        "        if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "        else:\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "        yield squad_eg.q_ids, pred_ans\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "def save(generator):\n",
        "    import json\n",
        "    with open('expert_prediction.json', 'w') as p:\n",
        "        json.dump(dict([i for i in generator]), p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5CwBdtXBU3j"
      },
      "source": [
        "# save(generate_prediction(electra_model, raw_eval_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEBkg0cN1kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487eb74f-3ffc-4e40-b3bf-18c1a4d536d5"
      },
      "source": [
        "save(generate_prediction(expert_model, test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create samples...\n",
            "Samples creation completed...\n",
            "Create input data...\n",
            "Time for prediction: 376.81891083717346s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRhAZ1tprd_X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b6e3c8e-9ff5-4a4a-e973-bb0ef746c6a2"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "folder = os.path.join('drive', 'MyDrive', 'saved_nlp_models')\n",
        "if not os.path.exists(folder):\n",
        "  os.makedirs(folder)\n",
        "shutil.move('expert_saved_model', folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/saved_nlp_models/expert_saved_model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PmhfRDKt6GH",
        "outputId": "5394f818-925b-4c25-9f2b-dbb5e651b99c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ctY_KPrSbg"
      },
      "source": [
        "def generate_prediction_(model, data):\n",
        "    # Stored results\n",
        "    res = {}\n",
        "    count = 0\n",
        "    ######\n",
        "    print('Create samples...')\n",
        "    sample_examples = create_squad_examples(data)\n",
        "    print('Samples creation completed...')\n",
        "    print('Create input data...')\n",
        "    x_eval, _ = create_inputs_targets(sample_examples)\n",
        "    # get the offsets of the first and last tokens of predicted answers\n",
        "    st = time.time()\n",
        "    pred_start, pred_end = model.predict(x_eval)\n",
        "    ed = time.time()\n",
        "    print(f\"Time for prediction: {(ed - st)}s\")\n",
        "    count = 0\n",
        "    pred_ans = None\n",
        "\n",
        "    eval_examples_no_skip = [_ for _ in sample_examples if _.skip == False]\n",
        "    # for every pair of offsets\n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "        # take the required Sample object with the ground-truth answers in it\n",
        "        squad_eg = eval_examples_no_skip[idx]\n",
        "        # use offsets to get back the span of text corresponding to\n",
        "        # our predicted first and last tokens\n",
        "        offsets = squad_eg.context_token_to_char\n",
        "        q_id = squad_eg.q_ids\n",
        "        # Get the best i.e max\n",
        "        start = np.argmax(start)\n",
        "        end = np.argmax(end)\n",
        "        if start >= len(offsets):\n",
        "            continue\n",
        "        pred_char_start = offsets[start][0]\n",
        "        if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "        else:\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\n",
        "        normalized_pred_ans = normalize_text(pred_ans)\n",
        "        normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "        if normalized_pred_ans not in normalized_true_ans:\n",
        "          count += 1\n",
        "          res[q_id] = normalized_pred_ans\n",
        "    return res\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8G3ED3dsnz7"
      },
      "source": [
        "import os\n",
        "model_name = ['expert_saved_model', 'baseline_saved_model', 'electra_saved_model']\n",
        "folder = os.path.join('drive', 'MyDrive', 'saved_nlp_models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dZzdQaNsdbe"
      },
      "source": [
        "model = tf.keras.models.load_model(os.path.join(folder, model_name[0]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKfnktVUtP4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6777b93c-bced-480b-f47b-a167799ad1cf"
      },
      "source": [
        "results = generate_prediction_(expert_model, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create samples...\n",
            "Samples creation completed...\n",
            "Create input data...\n",
            "Time for prediction: 376.8276176452637s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJtc87c8wC1o"
      },
      "source": [
        "with open('expert_wrongly_classified.json', 'w') as obj:\n",
        "  json.dump(results, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujQm5vhsLvN8"
      },
      "source": [
        "def get_raw_scores(dataset):\n",
        "    ground_truth = {}\n",
        "    for article in dataset['data']:\n",
        "        for p in article['paragraphs']:\n",
        "            for qa in p['qas']:\n",
        "                qid = qa['id']\n",
        "                gold_answers = [a['text'] for a in qa['answers']\n",
        "                                if normalize_text(a['text'])]\n",
        "                ground_truth[qid] = [qa['question'], gold_answers]\n",
        "    return ground_truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFgVHgXrN7vk"
      },
      "source": [
        "ground_truth = get_raw_scores(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywnu6U8cOP0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c82031-7d56-4a0f-d85b-9de25953c0a7"
      },
      "source": [
        "len(ground_truth.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25405"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUmjwltoOWdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f5898f-f978-4bdd-d485-79645d71cf6f"
      },
      "source": [
        "print(f\"Number of misclassified: {len(results.keys())}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of misclassified: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLDYE7yRjUSB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9qNI1TaOjPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd21b355-ce58-4c62-80cb-1554d9275105"
      },
      "source": [
        "for i, (k,v) in enumerate(ground_truth.items()):\n",
        "  if k in results:\n",
        "    print(f\"id: {k}\\nQuestion: {v[0]}\\nGround truth: {v[1][0]}\\nPrediction: {results[k]}\\n\")\n",
        "  if i==50:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id: 5728027d3acd2414000df20d\n",
            "Question: What year was the PlayStation 3 released?\n",
            "Ground truth: 2006\n",
            "Prediction: 2009\n",
            "\n",
            "id: 5728027d3acd2414000df210\n",
            "Question: What year did the Super Slim model hit stores?\n",
            "Ground truth: 2012\n",
            "Prediction: 2009 slim model of playstation 3 was released being lighter and thinner than original version which notably featured redesigned logo and marketing design as well as minor startup change in software super slim variation was then released in late 2012\n",
            "\n",
            "id: 572805603acd2414000df277\n",
            "Question: What shape was the Sixaxis prototype in?\n",
            "Ground truth: boomerang\n",
            "Prediction: boomerang shaped\n",
            "\n",
            "id: 572805603acd2414000df27a\n",
            "Question: What event did Sony take the PlayStation 3 to four months after E3?\n",
            "Ground truth: Tokyo Game Show\n",
            "Prediction: \n",
            "\n",
            "id: 572805603acd2414000df27b\n",
            "Question: What was one game Sony debuted on a modified PC so gamers could get a look?\n",
            "Ground truth: Metal Gear Solid 4: Guns of the Patriots\n",
            "Prediction: \n",
            "\n",
            "id: 5728066eff5b5019007d9b2b\n",
            "Question: By the time the system appeared at E3 2006, how many Ethernet ports was it down to?\n",
            "Ground truth: one\n",
            "Prediction: one ethernet port and four usb ports\n",
            "\n",
            "id: 572807bb3acd2414000df2b1\n",
            "Question: Which region experience a setback that pushed back the release of the PlayStation 3?\n",
            "Ground truth: PAL\n",
            "Prediction: pal region\n",
            "\n",
            "id: 572807bb3acd2414000df2b3\n",
            "Question: Which Japanese PS3 model got a 20%-plus price cut before hitting the market?\n",
            "Ground truth: 20 GB model\n",
            "Prediction: 20 gb\n",
            "\n",
            "id: 57280cc73acd2414000df319\n",
            "Question: What was the model number of the slim version of the PlayStation 3?\n",
            "Ground truth: CECH-2000\n",
            "Prediction: ps3 cech2000\n",
            "\n",
            "id: 57280cc73acd2414000df31c\n",
            "Question: What did the boot screen on the game consoles read before Sony changed it to \"PS3 PlayStation 3\"?\n",
            "Ground truth: \"Sony Computer Entertainment\"\n",
            "Prediction: \n",
            "\n",
            "id: 572810633acd2414000df386\n",
            "Question: What's the name of the sequel game to MotorStorm?\n",
            "Ground truth: MotorStorm: Pacific Rift\n",
            "Prediction: resistance 2 and motorstorm pacific rift\n",
            "\n",
            "id: 572814c34b864d190016441e\n",
            "Question: Which Ratchet & Clank title debuted at E3 2007?\n",
            "Ground truth: Ratchet & Clank Future: Tools of Destruction\n",
            "Prediction: ratchet clank\n",
            "\n",
            "id: 572814c34b864d190016441f\n",
            "Question: What year was Warhawk released for the PlayStation 3?\n",
            "Ground truth: 2007\n",
            "Prediction: \n",
            "\n",
            "id: 572814c34b864d1900164422\n",
            "Question: Which much anticipated third-party game with the name of a month of the year in it did Sony show at E3 2007?\n",
            "Ground truth: Devil May Cry 4\n",
            "Prediction: metal gear solid 4 guns of patriots\n",
            "\n",
            "id: 5728162d4b864d1900164440\n",
            "Question: What's Sony's budget line of PS3 games called in Japan?\n",
            "Ground truth: The Best\n",
            "Prediction: best range\n",
            "\n",
            "id: 5728162d4b864d1900164444\n",
            "Question: What words would you see in the United States or Canada on a PS3 game that would signify its lower price?\n",
            "Ground truth: Greatest Hits\n",
            "Prediction: greatest hits range\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3puQVD7bZn-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399341f9-2e15-4ed1-b8a4-a84e3152d0c2"
      },
      "source": [
        "print(f\"Proportion of wrongly classifier: {len(results.keys())/len(ground_truth.keys())}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportion of wrongly classifier: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwrxzSJgKl1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}